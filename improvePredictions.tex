\section{Advancing predictions of Hi-C interaction matrices}
In the following subsections, two conceptually different approaches towards the goal of the thesis,
predicting Hi-C matrices from ChIP-seq data, will be explored.
While the first approach is a dense neural network based on work by Farr\'e et al. \cite{Farre2018a},
the second is a novel method based on conditional generative adversarial networks (\acrshort{cgan}s).

\subsection{Dense Neural Network approach}\label{sec:improve:DNNapproach}
In their 2018 paper \cite{Farre2018a}, Pau Farr\'e, Alexandre Heurtau, Olivier Cuvier and Eldon Emberly
proposed a combination of a 1D convolutional filter with a three-layer dense neural network 
which already fulfills most goals of this thesis with some exceptions regarding data formats and preprocessing, 
cf. \cref{sec:prior:predictingInteractions} and \ref{sec:prior:discussion}.
This thesis attempts to build on the success of their method by extending the comparatively simple neural network
in various ways, changing the learning process and the loss functions in use.
As a start, the basic network has been rebuilt and used on well-known data from human cell lines GM12878 and K562, cf. \cref{sec:methods:input_data}.

\subsubsection{Basic network setup} \label{sec:improve:basicNetwork}
The basic network setup taken over from Farr\'e et al. \cite{Farre2018a} is shown in simplified form in \cref{fig:improve:priciple_basic_dnn},
see \cref{sec:methods:basicSetup} for technical details.
\begin{figure}[hbp]
    \small
    \centering
    \resizebox{\textwidth}{!}{
    \import{figures/}{explanation_dnn_hic.pdf_tex}}
    \caption{Principle of basic dense neural network}
    \label{fig:improve:priciple_basic_dnn}
\end{figure}

Since most organisms feature only a small number of chromosomes, 
it is generally infeasible to learn Hi-C matrices for full chromosomes at once due to a lack of training data.
Instead, the method by Farr\'e et al. employs a sliding window approach and learns from submatrices of a certain size, 
which allows for a reasonable number of training samples.
The target matrices are thus taken as overlapping submatrices of size $(w, w)$ 
with fixed window size $w$, centered at the diagonal of the 
original Hi-C matrices, \cref{fig:improve:priciple_basic_dnn} top.
The ChIP-seq data of $n$ chromatin features is taken as $(3w, n)$-subarray of the original array,
whereby the middle $w$ bins are aligned with the position of the submatrix,
the first $w$ bins correspond to the left flanking region and the last $w$ bins correspond to 
the right flanking region of the current submatrix region, \cref{fig:improve:priciple_basic_dnn} bottom. 
Samples are then obtained from the input data by sliding 
the input windows along the diagonal of the target Hi-C matrix with step size one. 
For predictions, overlapping samples can be re-assembled into complete matrices without difficulties
by taking the mean prediction for each genomic position.
Further technical details of the sample generation process are given in \cref{sec:methods:sample_gen}.

Within the network, a 1D convolutional filter compresses the $(3w, n)$-shaped input arrays to 1D vectors
of length $3w$, and four dense layers further process the compressed input, \cref{fig:improve:priciple_basic_dnn} middle.
The number of neurons in the last dense layer corresponds to the number of bins
in the upper triangular part of the target submatrix, i.\,e. it consists of $(w \cdot (w+1))/2$ neurons, \cref{fig:improve:priciple_basic_dnn} top, 
exploiting the symmetry of Hi-C matrices. 
For implementation details, please refer to \cref{sec:methods:basicSetup}.

Training of the network is performed by minimizing the mean squared error of the predicted matrix
versus the target Hi-C matrix using stochastic gradient descent, cf. \cref{sec:methods:basicSetup}.

Farr\'e et al. propose a window size of $w=80$ at $b_{feat} = b_{mat} = \SI{10}{\kilo\bp}$,
which comes close to the maximum bin distance available in the Hi-C data by Schuettengruber et al. \cite{Schuettengruber2014} used in their publication \cite{Farre2018a}.
However, larger bin sizes of $b_{feat} = b_{mat} = \SI{25}{\kilo\bp}$ were found beneficial for 
most of the data used throughout this master thesis.
Additionally, larger bin sizes allow for a higher coverage of the target matrix at the same window size,
because the window size is specified in bins, and obviously $10\, w < 25\, w$.
Prediction results from the initial network for windowsize $w=80$ and both bin size 10 and \SI{25}{\kilo\bp} are shown in \cref{sec:initialDNNresults}.

The network layout shown above is quite simple, and immediately offers some opportunities
for expansion, partially already proposed in the original paper \cite{Farre2018a}.
These will be explored below.

\subsubsection{Modifying the convolutional part of the network} \label{sec:improve:convolution_extensions}
One starting point for modifying the neural network is its convolutional part.

With only a single 1D convolutional filter in one layer, the network was expected to have difficulties capturing complex relationships 
between Hi-C interaction counts and more than one of the chromatin features.
For this reason, an extended ``longer'' network was created, 
comprising three 1D convolutional filter layers with 16, 8 and 4 filters, respectively, replacing the single
1D-convolution in \cref{fig:improve:priciple_basic_dnn}, lower left, cf. \cref{sec:methods:variants}.
This is still a comparatively low number of layers and filters,
but the choice seemed justified in order to avoid overfitting to the low-dimensional input.
The results are shown in \cref{sec:results:wider-longer-etc}, especially figures \ref{fig:results:longerDNN_pearson} and \ref{fig:results:longer_matrices}.

Next, a ``wider'' network was created, featuring the same setup as the basic network
except the width of the filter kernel, which was set to 4 instead of 1.
The idea here was to allow the network to capture correlations between Hi-C interaction counts
and chromatin features which span more than one bin. 
The actual number has been kept low, since at bin size $b=\SI{25}{\kilo\bp}$, 4 bins already correspond to \SI{100}{\kilo\bp}.
However, the results were not as expected, cf. \cref{sec:results:wider-longer-etc}, especially figures \ref{fig:results:widerDNN_pearson} and \ref{fig:results:wider_matrices}.
Of course, increasing filter width and using more filters can also be combined,
hopefully allowing the  ``wider-longer'' network to capture both correlations
spanning more than one bin and more than one chromatin feature.
The results for this combined approach are shown in \cref{sec:results:wider-longer-etc}, especially figures \ref{fig:results:wider-longerDNN_pearson} and \ref{fig:results:wider-longer_matrices}.

Another approach to potentially improve the predictions that goes somewhat into
the direction of the ``wider'' network has been proposed, but not implemented by Farr\'e et al. 
in their paper \cite{Farre2018a}.
Read counts from ChIP-seq experiments can usually be binned at smaller bin sizes than Hi-C data due to the nature of 
the processes. 
This can be exploited to capture finer details in the ChIP-seq data without a need for higher (training-)matrix resolutions.
To this end, the initial network can be generalized by binning the ChIP-seq data at $k$ times the bin size of the matrices, 
whereby $k \in \mathbb{N}^{\geq1}$, cf.~\cref{sec:methods:inputBinning} for the technical details.
This yields an input data size of $(k \cdot 3w, n)$, which is then again compressed to a vector of length $3w$ 
by a 1D convolutional filter with kernel size $n$ and strides $k$. 
To match given matrix bin sizes, $k=5$ was chosen for the thesis at hand, 
and the results for bin sizes $b_{mat}=\SI{25}{\kilo\bp}$, $b_{feat}=\SI{5}{\kilo\bp}$ are shown in \cref{sec:results:wider-longer-etc},
especially figures \ref{fig:results:25k5DNN_pearson} and \ref{fig:results:25k5_matrices}.

Since hardly any improvements were recorded for the modifications of the convolutional network part described above,
advanced loss functions were investigated next.

\subsubsection{Using a combination of MSE-, TV- and perceptual loss} \label{sec:improve:combined_loss}
In image regression tasks, optimizing for mean squared error (MSE) is known to produce blurry images,
because MSE is treating each image pixel individually, ignoring spatial proximity and structures in images \cite{Isola2017,Lu2019}.
And indeed, both the predictions from the initial and the extended network according to sections \ref{sec:improve:basicNetwork} and \ref{sec:improve:convolution_extensions} 
suffered from blurriness. 
To improve on this, investigations with modified loss functions were made. 

It has been shown that loss functions based on the (multiscale-)structural similarity index (SSIM) \cite{Wang2003}. 
can outperform mean squared error (L2) and mean absolute error (L1) in image regression tasks.
While Zhao et al. used a combination of L1- and multiscale SSIM loss \cite{Zhao2017},
Lu proposed a custom level-weighted SSIM loss \cite{Lu2019}.
The results were better than with L1- or L2-loss alone, but sometimes not much -- depending on the machine learning model in use.
Owing to difficulties in finding suitable parameters, SSIM-based losses were not considered for the thesis.

Another type of loss function used to produce sharp images is the so-called perceptual- or perception loss.
Here, the idea is to use a pre-trained perception network to determine structures in images and then compute for example L1- or L2-loss on these structures instead of images,
which has led to visually better predictions in some applications.
The process is shown in more detail in \cref{fig:improve:perceptual_loss}. 
\begin{figure}[hbt]
    \small
    \centering
    %\resizebox{\textwidth}{!}{
    \import{figures/}{explanation_perceptual_loss.pdf_tex}%}
    \caption{Perceptual loss}
    \label{fig:improve:perceptual_loss}
\end{figure}

As usual, training samples are fed into the network to be trained, and an output -- here, the predicted Hi-C submatrix -- is generated,
\cref{fig:improve:perceptual_loss}, orange path.
However, the loss function is then not computed on the predicted versus the true matrix. 
Instead, the activations of a selected layer in the perception network are generated for both the predicted Hi-C submatrix (orange path)
and the true Hi-C submatrix (gray path), and the loss function is computed on these activations (or perceptions).
The optimization of the target network's trainable weights can then be performed as usual, for example with stochastic gradient descent and backpropagation, 
simply keeping the weights of the perception network constant while optimizing the trainable weights for minimum perceptual loss.

Often, complex image classification networks like VGG-16 \cite{Simonyan2015} are taken as loss network, 
e.\,g. in the well-known style-transfer network by Johnson et al. \cite{Johnson2016},
because certain layers in these networks are known to have a correspondence with relevant structures in images.
It is possible to use multiple perceptions in a joint loss function \cite{Johnson2016} and combine perceptual losses with other losses, as shown below.

To check whether the given learning task benefits from using a perceptual loss, a custom combined loss function was generated,
consisting of mean squared error $L_\mathit{MSE}$ between true- and predicted matrices, perceptual loss $L_\mathit{VGG}$ based on the VGG-16 network 
and total variation loss $L_\mathit{TV}$ on predicted matrices to reduce noise in the output while preserving edges \cite{Rudin1992}. 
This choice was inspired by the custom loss function used by Hong et al. in their successful Hi-C super-resolution network \emph{DeepHiC} \cite{Hong2020},
which is otherwise not similar to the network used here.
In short, the combined loss function $L$ is shown in \cref{eq:combined_loss_short}. 
Here, the $\lambda$ are individual loss weights, see \cref{sec:methods:combined_loss} for details.
\begin{equation}
 L_\mathit{combined} = \lambda_\mathit{MSE} \; L_\mathit{MSE} + \lambda_\mathit{VGG} \; L_\mathit{VGG} + \lambda_\mathit{TV} \; L_\mathit{TV} \label{eq:combined_loss_short}
\end{equation}

Unfortunately, there is no straightforward way for determining the optimal parameters $\lambda$,
and an exhaustive parameter search was infeasible due to the computation time requirements of about 4:30\,min per epoch.
Therefore, only few runs were conducted with different sets of parameters,
and the results for $\lambda_\mathit{MSE} = 0.8999, \lambda_\mathit{VGG}=0.1, \lambda_\mathit{TV}=0.0001$,
which should not be considered optimal, are shown in \cref{sec:results:loss_functions}.

\subsubsection{Using a TAD-based loss function} \label{sec:improve:TAD_loss}
Looking at the results obtained from the networks discussed in the previous sections, see e.\,g. \cref{fig:results:basic500}, 
it seemed that highly interacting regions, especially topologically interacting domains (TADs), were not well predicted and either absent
in the matrix plots or blurred.
Assuming availability of a TAD scoring function $\mathit{tad}(\mathbf{z}_\mathit{pred})$, where $\mathbf{z}_\mathit{pred}$ is a predicted submatrix,
this might be improved by directly optimizing a loss function as shown in \cref{eq:combined_loss_tad}.
\begin{equation}
 L_\mathit{combined} = \lambda_\mathit{MSE}\;L_\mathit{MSE} + \lambda_\mathit{TAD} \, (\mathit{tad}(\mathbf{z}_\mathit{true}) - \mathit{tad}(\mathbf{z}_\mathit{pred}))^2 \label{eq:combined_loss_tad}
\end{equation}
However, this approach suffers from several restrictions.
First and foremost, there seems no consensus on the exact definition of TADs, 
and no less than 22 algorithms for TAD detection existed as of 2018 \cite{Dali2017,Zufferey2018}.
Additionally, many of these algorithms have several tuning parameters, are notoriously parameter-dependent
and may not even yield any results if parametrized in an unfavorable way \cite{Zufferey2018}. 
A further restriction results from the context -- since the loss function needs to be optimized,
one needs to be able to compute gradients of it with respect to the network's weights.
Due to their complexity, this is generally very difficult to implement in a computationally efficient way for all known TAD calling algorithms.

To overcome the limitations, a novel loss function based on TAD scores \cite{Crane2015} was developed.
\Cref{fig:improve:tad_score_loss_function} exemplarily shows its basic idea for a $(16,16)$-shaped submatrix
with window size 4.
\begin{figure}[hbt]
 \begin{minipage}{0.65\textwidth}
   \resizebox{\textwidth}{!}{
    \small
    \import{figures/}{tad_score_loss_function.pdf_tex}}
    \caption{Score vector generation for TAD-based loss}
    \label{fig:improve:tad_score_loss_function}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item first ``diamond''
    \item first 4 diagonals of a 16 by 16 Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item sliding direction for score generation
    \item score vector (mean values within diamonds)
    \item line plot of score vector
    \item possible TAD boundaries
\end{enumerate}
 \end{minipage}
\end{figure}
First, the mean is taken from diamond-shaped (or rhombus-shaped) matrix cutouts along the diagonal, \cref{fig:improve:tad_score_loss_function}\,(A, C),
and stored in a score vector, \cref{fig:improve:tad_score_loss_function}\,(D / E). 
The size of the diamonds, 2 in the figure, is configurable, but a reasonable balance with the submatrix size, i.\,e. the size of the sliding window $w$,
and the expected size of TADs in the matrix must be maintained.
Next, loss is computed by taking the mean squared error between the score vectors of the ``real'' submatrices and the ``predicted'' submatrices, \cref{eq:combined_loss_tad}.

The idea behind the score-based approach is visualized in \cref{fig:improve:tad_score_loss_function}\,(E).
Local minima, i.\,e. dents in the line plot of the score values, \cref{fig:improve:tad_score_loss_function}\,(F),
often correspond with highly interacting regions in the matrix, since the mean of diamonds from \emph{inside} TADs is normally significantly higher than the mean of diamonds \emph{outside} TADs.
Indeed, some TAD calling algorithms like \emph{TopDom} \cite{Shin2015} and \emph{hicFindTADs} \cite[W12f.]{Wolff2018} do compute insulation scores -- usually for more than one diamond size -- 
and then use diverse techniques to detect meaningful local minima in the score values.
However, finding meaningful local minima in the given context is still computationally involved,
so it was left to the network to make sense out of the score vectors.
This way, the loss function was well defined, efficiently computable and tensorflow standard functionality could be used to compute gradients with respect to weights.

For the thesis at hand, window size $w=80$ and diamond size $ds={12}$ were used at bin sizes of \SI{25}{\kilo\bp}.
For technical details please refer to \cref{sec:methods:score_loss}.

\subsubsection{Modifying bin size and window size}\label{sec:improve:binsize_winsize}
In the results presented so far, larger structures were often absent.
In order to improve on this, two approaches were tried.

First, predictions were made at bin sizes $b_\mathit{feat} = b_\mathit{mat} = \SI{50}{\kilo\bp}$, 
keeping the window size at 80 bins, which then corresponded to \SI{4}{\mega\bp}.
The idea here was to capture predominantly larger structures further away from the diagonal and
then investigate various methods to combine predictions at smaller and larger bin sizes.
However, such a merging process was never actually developed,
since the predictions at \SI{50}{\kilo\bp} alone were not good enough, \cref{sec:results:binsize_winsize}.

Unfortunately, doubling the (matrix-)bin sizes from 25 to \SI{50}{\kilo\bp} directly leads to a reduction in the number of available training samples by a factor of about two, 
if the window sizes are kept constant at 80 bins, see \cref{tab:methods:samples}. 
This might also be one of the causes why predictions at \SI{50}{\kilo\bp} proved useless.
For this reason, a second approach was made -- using training samples at $b_\mathit{feat} = b_\mathit{mat} = \SI{25}{\kilo\bp},\;w=80$
and $b_\mathit{feat} = b_\mathit{mat} = \SI{50}{\kilo\bp},\;w=80$ at the same time.
This is easily possible, since the neural network topology only depends on the window size \emph{in bins} and the relation $k=b_\mathit{mat}/b_\mathit{feat}$
between the bin sizes of features and matrices.
This approach obviously increases the number of training samples,
with the idea of allowing the network itself to figure out how to combine predictions at smaller and larger bin sizes.


\begin{itemize}
 \item use trapezoids, i.e. capped larger submatrices and flanking size smaller than window size
 \item rationale: larger window size without increasing training time too much
 \end{itemize}
 
 

\subsubsection{DNA sequence as an additional network input branch}
\begin{itemize}
 \item use DNA as an additional input
 \item rationale a): allow the network to figure out true binding sites in conjunction with cs data
 \item rationale b): given the success of pure DNA based methods, allow the network to find yet unknown sequence structure correlations
 \item probably not the most important subsection, leave it out in case of time problems
\end{itemize}

\subsection{Hi-cGAN approach} \label{sec:improve:Hi-cGAN}
Because none of the results from the dense neural network approach presented in \cref{sec:results:DNN} were overly convincing,
a second, widely unrelated approach was made.

Since their invention by Goodfellow, Mirza and colleagues \cite{Goodfellow2014, mirza2014},
Generative Adversarial Networks have become increasingly popular for image processing tasks of many kinds,
and especially for image synthesis \cite{Wang2020}. 
Among their strengths is image synthesis from textual descriptions \cite{Reed2016,Zhang2019c,Zhu2019,Tao2020} --
and from an abstract point of view, this task is not very different from the goal of the thesis at hand.
Considering the chromatin features on the input side as a ``description'' of how the target Hi-C (sub-)matrices should look like,
formulating the goal of the thesis as a \acrshort{cgan}-problem should be possible.
In the following sections, such an approach will be explored.

\subsubsection{General setup of the cGAN approach}
Although numerous variants exist, conditional GANs generally comprise at least two neural networks -- 
a generator $G(x,z)$, which tries to generate realistic outputs from its inputs $x$ and random noise $z$, and a discriminator $D(x,y)$,
which tries to discern true inputs $y$, e.\,g. experimentally derived Hi-C matrices, from generated inputs $G(x,z)$.
The optimal weights for the networks can then be found by searching an equilibrium in a two-player minimax game:
The generator tries to fool the discriminator, while the discriminator tries to detect the generator's fakes, 
see equations \ref{eq:improve:cGAN-simple-loss} and \ref{eq:improve:cGAN-simple-opt} \cite{Isola2017}, where $\mathbb{E}$ is the expected value for each ``player''.
\begin{align}
 L_\mathit{cGAN}(G, D) &= \mathbb{E}_{x,y}[\log D(x,y)] + \mathbb{E}_{x,z}[\log(1-D(x, G(x,z)))] \label{eq:improve:cGAN-simple-loss} \\
 G^* &=  \mathit{arg\,min}_G \mathit{max}_D \; L_\mathit{cGAN}(G, D) \label{eq:improve:cGAN-simple-opt}
\end{align}

Many different layouts and training processes for the Generator and Discriminator networks have been proposed.
For the thesis at hand, the well-known \emph{pix2pix} proposal by Isola et al. from 2017 was followed \cite{Isola2017}, 
amended by a convolutional embedding network for the chromatin features, which serve as the conditional input $x$ here.
The overall setup is shown in \cref{fig:improve:cGAN-approach} and will be explained in more detail below.
\begin{figure}[htbp]
 \import{figures/}{Hi-cGAN.pdf_tex}
 \caption{\acrshort{cgan} approach} \label{fig:improve:cGAN-approach}
\end{figure}

The generator architecture is based the well known U-Net architecture due to Ronneberger, Fischer and Brox \cite{Ronneberger2015},
while the discriminator is implemented as a patchGAN-discriminator developed by Isola et al. especially for \emph{pix2pix} \cite{Isola2017}.
This type of discriminator splits the images into patches and tries to decide which of them are real and which are fake, using convolutions.
Note that \emph{pix2pix} does not explicitly add noise $z$, but by design only relies on dropout layers for that purpose. 

For the thesis at hand, few modifications were made to the actual \emph{pix2pix}network.
Since Hi-C matrices are symmetric by definition, symmetry of the outputs was enforced
by adding additional layers to the network in the appropriate places, see \cref{sec:methods:cGAN_initial} for details.
Furthermore, some layers in the generator and discriminator were made optional to allow processing smaller images of sizes $64\times64$
and $128\times128$ aside the $256\times256$-images of the original implementation, again refer to \cref{sec:methods:cGAN_initial} for details.

For sample generation, the same sample generation method as for the dense neural network described above was employed, 
yet with a few minor adaptations, see \cref{sec:methods:sample_gen_cgan}.
However, since \emph{pix2pix} operates on images, as its name already implies, 
a method to add the essentially one-dimensional chromatin feature data as conditional input ``images'' into the network was needed.
Unfortunately, this task seems to be without examples in literature, so two different embedding approaches were made,
which will be discussed below in sections \ref{sec:improve:DNN_embedding} and \ref{sec:improve:DNN_embedding}.
Both approaches are distantly related to the text-encoders used by text-to-image synthesis networks 
\cite{Reed2016b,Xu2018}, but much less sophisticated. 
However, a complex encoding is not needed here, since -- contrary to image captions -- the chromatin features are already in a format that can be processed directly by neural networks.

Many ways for training a \acrshort{cgan} exist, and achieving a stable, converging training process can be tricky.
For the thesis at hand, the generator loss function proposed by Isola et al. \cite{Isola2017} was used, 
amended by a TV loss function for noise reduction, \cref{eq:improve:cGAN_loss}.
\begin{align}
 L_G(\mathbf{x}, \mathbf{y}, \mathbf{z}) &=\lambda_\mathit{MAE}L_\mathit{MAE}(G(\mathbf{x},\mathbf{z}), \mathbf{y}) + \lambda_\mathit{adv} \; L_d(\mathbf{1}, D(\mathbf{x}, G(\mathbf{x},\mathbf{z})) +  \lambda_\mathit{TV}L_\mathit{TV}(G(\mathbf{x},\mathbf{z})) \label{eq:improve:cGAN_loss}\\
 L_d(\mathbf{r, s}) &= \frac{1}{bs}\sum^{bs} \frac{1}{n^2}\sum^{n^2}\left( -\log(\mathrm{sigmoid}(\mathbf{s})) \cdot \mathbf{r} - \log(\mathbf{1} - \mathrm{sigmoid}(\mathbf{s})) \cdot (\mathbf{1} - \mathbf{r}) \right) \label{eq:improve:disc_loss1}\\
 \mathrm{sigmoid(m_{i,j,k})} &= \frac{e^{m_{i,j,k}}}{1+e^{m_{i,j,k}}} \quad \mathrm{for}\; i \in (0, 1, \dots, bs)\; \textrm{and}\; j,k \in (0, 1, \dots, n) \label{eq:sigmoid}
\end{align}
Here, $L_\mathit{MAE}$ is the mean absolute error (L1 error) between artificially generated output $G(x,z)$ and true Hi-C submatrices $y$ (as grayscale images), 
$L_d$ is the discriminator loss according to \cref{eq:improve:disc_loss1}, in this case with respect to artificially generated matrices, and 
$l_\mathit{TV}$ is a total variation loss as in \cref{eq:combined_loss_short} and \cref{eq:methods:tv}.
Since the discriminator is a patchGAN discriminator, it is working on batches which contain $bs$ image patches of shape $(n,n)$, and $\mathbf{1}$ is also a tensor of shape $(n,n)$, populated with ones.
To obtain a scalar loss value, the mean was taken across both batches and patches. 
The sigmoid function was computed individually for all elements $m_{i,j,k}$ in the $(bs,n,n)$-shaped tensors as shown in \cref{eq:sigmoid}, transforming them into value range [0...1].
Finally, individual scalar factors $\lambda$ were used to balance the influence of the individual loss portions on the joint loss function, cf. \cref{sec:methods:cGAN_initial}.

As already indicated in \cref{fig:improve:cGAN-approach}, the discriminator was trained as proposed by Isola et al.,
meaning that the binary cross entropy loss was optimized with respect to true- and fake samples, \cref{eq:improve:disc_loss_total} .
\begin{align}
 L_D(\mathbf{x,y,z}) = \lambda_D * \left[ L_d(\mathbf{1}, D(\mathbf{x,y})) + L_d(\mathbf{0}, D(\mathbf{x},G(\mathbf{x, z})) \right] \label{eq:improve:disc_loss_total}
\end{align}
In \cref{eq:improve:disc_loss_total}, $L_d$ is again the PatchGAN loss defined above in \cref{eq:improve:disc_loss1}, $y$ are true Hi-C submatrices, 
$G(x,z)$ are Hi-C submatrices artificially generated by the generator $G$
from chromatin feature data $x$ and random noise $z$, and $\mathbf{1}, \mathbf{0}$ are tensors of the appropriate shape with all ones and all zeros, respectively.
$\lambda_D$ is a scalar factor proposed by Isola et al. to slow down the learning process of the discriminator compared to the generator.

The original idea behind combining mean absolute error $L_\mathit{MAE}$ and PatchGAN adversarial loss $L_d$ is that optimizing MAE 
mainly minimizes low-frequency errors, while optimizing the PatchGAN-loss with its small patches helps reducing high-frequency errors \cite{Isola2017}.
The TV loss has been added for edge-aware noise removal, in line with other works in the field \cite{Hong2020}, cf.~\cref{sec:improve:combined_loss}.

Both the generator- and discriminator loss functions were optimized with the Adam optimizer,
alternately training discriminator and generator, see \cref{sec:methods:cGAN_initial} for details.

Compared to other \acrshort{cgan} architectures, \emph{pix2pix} is rather simple, but well studied for different applications like label-to-image transfer,
sketch-to-image transfer, grayscale-to-color transformations or image infill tasks \cite{Isola2017}.
Since the problem at hand has likely not been tackled by a tailor-made GAN yet, 
choosing a ``general-purpose'' \acrshort{cgan} as a starting point seemed reasonable.
Another advantage of \emph{pix2pix}, and U-Net architectures in general, 
is that they can achieve good performance even with comparatively few training samples \cite{Isola2017, Ronneberger2015}.

\subsubsection{Using a DNN for feature embedding} \label{sec:improve:DNN_embedding}
The embedding network required for processing the conditional inputs
has on obvious mandatory property: it must be capable of embedding the input of shape $(3w,n)$ 
into a 2D grayscale image of shape $(w,w,1)$ which can then be processed by a \emph{pix2pix}-like network.
Naturally, this can be achieved by using the dense neural network explored above
and reshaping its ``upper triangular'' output vector back into a symmetric ``image''-tensor with the required shape.

The rationale here was to first use the DNN to get a coarse estimate of the predicted matrix,
and then the \acrshort{cgan} to refine the results, somewhat similar to the two-stage approach by Zhang et al. \cite{Zhang2019c},
albeit much less sophisticated. 
Naturally, this approach also allows to pre-train the dense network as discussed above in \cref{sec:improve:DNNapproach}, 
this time for window size $w=64$ and then use transfer learning to provide better starting values for the weights of the embedding network, 
potentially improving both stability and convergence speed of the \acrshort{cgan} network.

A disadvantage of the dense network approach is that the number of neurons in its output layer quadratically depends on the window size $w$,
cf. \cref{sec:improve:basicNetwork} and \ref{sec:methods:basicSetup}.
Taken together with the three fully connected layers underneath, this leads to a superlinear increase in the number of parameters along with $w$,
further aggravated by the fact that the \acrshort{cgan} requires $w$ to be powers of two, cf. \cref{sec:methods:cGAN_initial} and  
\ref{sec:methods:dnn-embedding} (\cref{tab:methods:embedding_network_params}, p.~\pageref{tab:methods:embedding_network_params}).
The DNN embedding has thus only been explored for $w=64$, both with and without pre-training the DNN. 
The results are to be found in \cref{sec:results:cgan_dnn} and some more technical details are given in \cref{sec:methods:dnn-embedding}.

\subsubsection{Using a CNN for feature embedding} \label{sec:improve:CNN_embedding}
Since the results from the \acrshort{cgan} with DNN embedding were not convincing, another embedding network was designed, based on convolutional layers.
Here, the idea was to have a trainable embedding that keeps the localization information contained in the chromatin feature data,
has less parameters than a dense embedding and can efficiently be trained along with the rest of the network even at window sizes up to 256 bins.

Since no example for such an embedding was found in literature, a first attempt was made with a simple convolutional neural network (CNN).
This network essentially featured 8 building blocks, each consisting of 1D convolution, batch normalization layer to avoid overfitting and leaky ReLU activation \cite{Maas2013}
to avoid the so-called ``dying ReLU'' problem. 
A final 9th convolution layer was added to ensure the required output shape. 
The complete setup is shown in \cref{sec:methods:cnn-embedding} (\cref{fig:methods:GAN_arch:embedding_network}, p.~\pageref{fig:methods:GAN_arch:embedding_network}).
Compared to the dense approach, the number of trainable parameters in the CNN embedding is widely independent of the window size and 
stays within 4.24...4.29 million parameters for the investigated window sizes 64, 128 and 256, cf. \cref{tab:methods:embedding_network_params} (p.~\pageref{tab:methods:embedding_network_params}).

The results of the \acrshort{cgan} with CNN embedding are shown in \cref{sec:results:cgan_cnn} and were indeed better than the ones obtained with DNN embeddings.

\subsubsection{Using mixed DNN- and CNN-embedding for feature embedding}
Besides using a DNN- or CNN-embedding for \emph{both} generator and discriminator, it is obviously also possible
to use different embedding network types.
To investigate the influences of the embedding network on the general performance,
a ``mixed'' embedding, i.\,e. a DNN-embedding for the generator and a CNN-embedding for the discriminator was tested,
using the same DNN and CNN as above.
The results are shown in \cref{sec:results:cgan-mixed}.











