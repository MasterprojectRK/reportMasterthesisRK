\section{Advancing predictions of Hi-C interaction matrices}
In the following subsections, two conceptually different approaches towards the goal of the thesis,
predicting Hi-C matrices from ChIP-seq data, will be explored.
While the first approach is a dense neural network based on work by Farr\'e et al. \cite{Farre2018a},
the second is a novel method based on conditional generative adversarial networks.

\subsection{Dense Neural Network approach}\label{sec:DNNapproach}
In their 2008 paper \cite{Farre2018a}, Pau Farr\'e, Alexandre Heurtau, Olivier Cuvier and Eldon Emberly
propose a combination of a 1D convolutional filter with a three-layer dense neural network 
which already fulfills most goals of this thesis with some exceptions regarding data formats and preprocessing.
Here, we try to build on the success of their method by extending the comparatively simple neural network
in various ways, modifying the binsizes of the Hi-C matrices and changing the learning process.

\subsubsection{Basic network setup} \label{sec:improve:basicNetwork}
The basic network setup taken over from Farr\'e et al. \cite{Farre2018a} is shown in simplied form in \xxx.

Since the network implements a supervised learning technique,
it requires two kinds of inputs for training -- ChIP-seq data of the chosen chromatin factors and
target Hi-C matrices for each training chromosome.
The target matrices are just taken as submatrices of size $w \times w$ 
with fixed windowsize $w$, centered at the diagonal of the 
original Hi-C Matrix, cf. \cref{sec:methods:hicMatrices}.
The ChIP-seq data is taken as $3w \times n$ subarray of the original array, cf. \cref{sec:methods:chipseq},
whereby the middle $w$ bins are aligned with the position of the submatrix,
the first $w$ bins correspond to the left flanking region and the last $w$ bins correspond to 
the right flanking region of the current submatrix region. 
Training samples are then obtained from the input data by sliding 
the input windows along the diagonal of the target Hi-C matrix, \xxx.

Within the network, a 1D convolutional filter compresses the $3w \times n$ inputs to a 1D vector
of size $3w \times 1$, and three dense layers further process the compressed input.
The number of neurons in the last dense layer corresponds to the number of bins
in the upper triangular part of the target submatrix, i.\,e. it consists of $(w \cdot (w+1))/2$ neurons.

Training of the network is performed by minimizing the mean squared error of the predicted matrix
versus the target Hi-C matrix using stochastic gradient descent.
The technical details are given in \cref{sec:methods:denseNN}
and the initial results for our basic setup are shown in \cref{sec:initialDNNresults}.

The network shown above is quite simple, and immediately offers some opportunities
for expansion, partially already proposed in the original paper \cite{Farre2018a}.
These will be explored below.

\subsubsection{Modifying the convolutional part of the network}
One starting point for modifying the neural network is its convolutional part.

With only a single 1D convolutional filter in one layer, there is little chance of capturing complex relationships 
between Hi-C interaction counts and more than one of the chromatin features.
For this reason, an extended ``longer'' network has been created, 
comprising three 1D convolutional filter layers with 16, 8 and 4 filters, respectively.
This is still a comparatively low number of layers and filters,
but the choice seems justified in order to avoid overfitting to the low-dimensional input.

Next, a ``wider'' network was created, featuring the same setup as the basic network
except the width of the filter kernel, which was set to 4 instead of 1.
The idea here is to allow the network to capture correlations between Hi-C interaction counts
and chromatin features which span more than one bin. 
The actual number has been kept low, since at binsize $b=\SI{25}{\kilo\bp}$, 4 bins already correspond to \SI{100}{\kilo\bp}.
Of course, increasing filter width and using more filters can also be combined,
hopefully allowing the  ``wider-longer'' network to capture both correlations
spanning more than one bin and more than one chromatin feature.

Another approach to potentially improve the predictions that goes somewhat into
the direction of the ``wider'' network has been proposed, but not implemented by Farr\'e et al. 
in their paper \cite{Farre2018a}.
As already noted in \cref{sec:intro:chipseq}, the ChIP-seq data can usually
safely be binned at smaller binsizes than Hi-C data due to the nature of 
the process. 
This can be exploited to capture finer details in the ChIP-seq data without a need for higher (training-)matrix resolutions.
To this end, the initial network can be modified by binning the ChIP-seq data at $k$ times the bin size of the matrices, 
whereby $k \in \mathbb{N}^{\geq1}$. 
This yields an input data size of $k \cdot 3w \times n$, which is then again compressed to a $3w \times 1$ vector 
by a 1D convolutional filter with kernel size $n$ and strides $k$. 
The basic network from \cref{sec:improve:basicNetwork} can be seen as a special case of this more general setup
with $k=1$. 
For practical reasons, $k=5$ was chosen for the thesis at hand, 
and the results for binsize $b_{mat}=\SI{25}{\kilo\bp}$ are shown in \xxx.

\subsubsection{Modifying the loss function}
\begin{itemize}
 \item MSE is known to produce blurry images in regression tasks
 \item others have used TV loss, Perception loss to improve on this
 \item SSIM can also be used as to improve perception, but caution needed
 \item additionally, an insulation-score-based approach to get sharper boundaries 
            at highly interacting regions
\end{itemize}

\subsubsection{Modifying binsize and windowsize}
\begin{itemize}
 \item rationale: smaller binsizes - predict smaller structures, larger binsizes - predict larger structures
 \item then combine the predictions (e.g. sum them up, take the mean or even use a NN)
 \item use trapezoids, i.e. capped larger submatrices
 \item rationale: larger windowsize without increasing training time too much 
 \end{itemize}

\subsubsection{DNA sequence as an additional network input branch}
\begin{itemize}
 \item use DNA as an additional input
 \item rationale a): allow the network to figure out true binding sites in conjunction with cs data
 \item rationale b): given the success of pure DNA based methods, allow the network to find yet unknown sequence structure correlations
 \item probably not the most important subsection, leave it out in case of time problems
\end{itemize}

\subsection{Hi-cGAN approach} \label{sec:hi-cGAN}
\subsubsection{Pix2Pix as a generic generative network}
Describe the basic setup of pix2pix and what it is currently used for
Pix2Pix using 2D input, but we have n times 1D. Bad. 2 ideas how to come around,
using DNN like Farre et al, potentially pretrained, or using new CNN. Maybe mention restriction of $2^x$ for image size
\subsubsection{Using a DNN for 1D - 2D conversion}
\begin{itemize}
\item use network like Farre et al for converting inputs to 2D
\item rationale: this nw has been shown to create comparatively good Hi-C matrices 
\item rationale: pix2pix could then improve them
\item rationale: transfer learning can be employed
\end{itemize}

\subsubsection{Using a CNN for 1D - 2D conversion}
\begin{itemize}
 \item use somewhat deep CNN for conversion
 \item rationale: the DNN approach didn't work
 \item rationale: downsampling like in image processing, just in 1D
\end{itemize}















