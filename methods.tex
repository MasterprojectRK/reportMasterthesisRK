\section{Method details}

\subsection{Input data}
For the thesis at hand, data from human cell lines GM12878, K562, HMEC, HUVEC and NHEK was used.
The exact data sources and data preprocessing will be outlined in the following subsections \ref{sec:methods:hicMatrices} and \ref{sec:methods:chipseq}.


\subsubsection{Hi-C matrices} \label{sec:methods:hicMatrices}
Hi-C data due to Rao et al. \cite{Rao2014} was downloaded 
in .hic format from Gene Expression Omnibus under accession key GSE63525.
Here, the quality-filtered ``combined\_30'' matrices were taken, which contain only high-quality reads from 
both replicates.

Next, matrices at \SI{5}{\kilo\bp} binsize were extracted and converted to cooler format using \texttt{hic2cool}
and subsequently coarsened to resolutions of 10, 25, 50 and \SI{100}{\kilo\bp} using \texttt{cooler coarsen}.

Contrary to the work from Farr\'e et al. \cite{Farre2018a}, which is using a distance-based normalization,
and many others in the field which are using ICE- or KR-normalization, 
these matrices have not been normalized for the thesis at hand
because no benefit of doing so was found during the study project \cite{Krauth2020}.

In the cGAN approach outlined in \cref{sec:hi-cGAN}, the matrices are essentially treated as images and were
thus scaled to a value range of [0...1] in 32-bit floating point format.

\subsubsection{ChIP-seq data} \label{sec:methods:chipseq}
For this thesis, ChIP-seq data for 13 chromatin features and DNaseI-seq data was used, cf.~\cref{tab:methods:csdata}.
Here, ChIP-seq data for the 13 features was downloaded in .bam format from the ENCODE project \xxx for both replicate one and two,
and DNaseI-seq data was downloaded in .bam format from \xxx; the download links are also given in \cref{tab:methods:csdata}.
\begin{table}[ht!]
\centering
 \begin{tabular}{ll}
 \hline
  feature name & download link \\  \hline
  CTCF & \\
  DNaseI &\\
  H3k27ac & \\
  H3k27me3 &\\
  H3k36me3 &\\
  H3k4me1 &\\
  H3k4me2 &\\
  H3k4me3 &\\
  H3k79me2 &\\
  H3k9ac &\\
  H3k9me3 &\\
  H4k20me1 &\\
  Rad21 &\\
  Smc3 & \\ \hline
 \end{tabular}
 \caption{chromatin features used for the thesis} \label{tab:methods:csdata}
\end{table}

The data were then converted to bigwig format, which is more convenient to handle, and the replicates were merged into 
one bigwig file. Pseudocode for the full conversion process is given in \xxx.

\begin{itemize}
 \item ENCODE as source
 \item Scaling to [0...1] for processing
 \item refrain from whitening, etc. -- the data is no image with inherent correlations
\end{itemize}

\subsubsection{Sample generation process} \label{sec:methods:sample_gen}
With machine learning approaches, success heavily depends on the availability of training data.
Since many organisms feature only a small number of unique chromsomes, learning Hi-C matrices 
for complete chromosomes at once is generally infeasible.
This thesis thus follows a sliding window approach, as proposed by Farr\'e et al. \cite{Farre2018a}.

First, all chromatin features were binned to binsize $b_{feat}$ by splitting each chromosome of size $cs$ into 
$l_{feat}=\left \lceil{\frac{cs}{b_{feat}}}\right \rceil -1$ non-overlapping bins of size $b_{feat}$
and taking the mean feature value within the genomic region belonging to each bin.
All $n$ chromatin factors were processed in this way and then stacked into a $l \times n$ array.

Hi-C matrices were taken as provided by the cooler format, cf. \cref{sec:methods:hicMatrices}, 
i.\,e. as $(l_{mat} \times l_{mat})$-matrices, $l_{mat}=\left \lceil{\frac{cs}{b_{mat}}}\right \rceil -1$ being the number of bins in the given chromosome. 
Initially, $b_{feat} = b_{mat}$ was used, which leads to $l_{feat} = l_{mat}$, because $cs$ is a constant for a given chromosome.

A sliding window approach was now applied to generate training samples for the networks $G$ described below.
Here, subarrays of size $(3w_{mat} \times n)$ were cut out from the feature array 
such that the $i$-th training sample corresponded to the subarray containing the columns $i,\,i+1,\,i+2,\,\dots,\,i+3w_{mat}$ of the full array. 
Sliding the window along the array with stepsize one obviously yields $N=l-3w_{mat}+1$ training samples.
The corresponding Hi-C matrices were then cut out along the diagonal of the original matrix 
as submatrices with corner indices [$j,\;j$], [$j,\;j+w_{mat}$], [$j+w_{mat},\;j+w_{mat}$], [$j+w_{mat},\;j$] in clockwise order, where $j=i+w_{mat}$.
The idea here is that the first $0,\,1,\,\dots \,w_{mat}$ columns of each feature sample form the left flanking region of the training matrix, 
the next $w_{mat}+1,\,w_{mat}+2,\,\dots \,2w_{mat}$ correspond to the matrix' region and the last $2w_{mat}+1,\,2w_{mat}+2,\,\dots \,3w_{mat}$ columns form the right flanking region.
Since Hi-C matrices are symmetric by definition, again only the upper triangular part of the submatrices was used, 
flattened into a $w\cdot (w+1)/2$ vector.

\Cref{fig:methods:sample_gen} exemplarily shows the sample generation process for a $(15\times15)$-matrix with $w_{mat}=4$ and $n=3$ chromatin features.
In this case, five training samples would be generated -- the one encircled in green and four more to the right, as the window is sliding from left to right until the
right flanking region hits the feature array boundary.
\begin{figure}[hbp]
 \begin{minipage}{0.60\textwidth}
   \centering
    \small
    \import{figures/}{sample_generation_process.pdf_tex}
    \caption{Sample generation process}
    \label{fig:methods:sample_gen}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin feature array
    \item sliding direction for sample generation windows
    \item first 4 diagonals of a $15\times15$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item first $4\times4$training matrix, corrsponding to feature window A-B-C
\end{enumerate}
 \end{minipage}
\end{figure}

The sample generation process for predicting (unknown) matrices was the same as for training,
except that no matrix window was generated.
Due to the sliding window approach, the output of the network is a set of overlapping submatrices along the main diagonal of the actual target Hi-C matrix.
To generate the final submatrix, all submatrices were added up in a position-aware fashion 
and finally all values were divided by the number of overlaps for their position.
Furthermore, the output matrices were scaled to value range [0...1000] for better visualisation and then stored in cooler format.
\Cref{fig:methods:prediction} exemplarily shows the prediction process for $N=5$ samples with windowsize $w=4$ for a $15\times15$ matrix.
Note that the first and last $w$ bins in each row (matrix diagonal) always remain empty due to the flanking regions,
as do all bins outside the main diagonal and the first $w-1$ side diagonals.
\begin{figure}
 \begin{minipage}{0.60\textwidth}
   \centering
    \small
    \import{figures/}{prediction_process2.pdf_tex}
    \caption{Prediction process}
    \label{fig:methods:prediction}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin feature array
    \item sliding direction for predicted submatrices
    \item first 4 diagonals of a predicted $15\times15$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item first predicted $4\times4$ matrix
\end{enumerate}
\raggedright{gray backround colors symbolize number of overlapping predictions at that position (bright = 1, dark = 4)}
 \end{minipage}
\end{figure}

The network and its learning algorithm was implemented in python using tensorflow deep learning framework with keras \xxx citation, see \xxx repository.
The basic network was used with parameters $n=14$, $w=80$ and $b_{feat}=b_{mat} = \SI{25}{\kilo\bp}$. 

Training samples were drawn from chromosomes 1, 2, 4, 7, 9, 11, 13, 14, 16, 17, 18, 20 and 22 of GM12878, 
validation samples from GM12878, chromosomes 6, 8, 12 and 15 and test samples from K562, chromosomes 3, 5, 10, 19 and 21,
see table \xxx for details.
Test samples were processed through the network and the resulting submatrices were subsequently re-assembled into full Hi-C matrices in cooler format.

\subsubsection{Generalization of feature binning} \label{sec:methods:inputBinning}
Most of the binning- and sample generation procedures described above 
also work for binsize relations $k=\frac{b_{mat}}{b_{feat}} \in \mathbb{N}^{>1}$.

The training matrices remain unchanged, i.\,e. $(l \times l)$-arrays, from which training submatrices of size  $w_{mat} \times w_{mat} $
can be extracted. 
With $k \in \mathbb{N}^{>1}$, one bin on the matrix diagonal corresponds to $k$ bins of the feature array,
so the feature windowsize needs to be $k$ times the submatrix windowsize, $w_{feat} = k \cdot w_{mat}$.
Since the first layer of all neural networks used in this thesis is a 1D convolution,
this can be achieved by setting the filter width and strides parameters of the (first) convolutional layer to $k$, leaving the rest of the network unchanged.
However, the number of bins along the matrix diagonal is generally not $k$ times the number of bins in the feature array,
see \cref{eq:methods:binning}.
\begin{equation}
 l_{feat} = \left \lceil{\frac{cs}{b_{feat}}}\right \rceil -1 
                = \left \lceil{\frac{cs}{k \cdot b_{mat}}}\right \rceil -1 
                \not = k \cdot (\left \lceil{\frac{cs}{ b_{mat}}}\right \rceil -1)
                = k \cdot l_{mat} \label{eq:methods:binning}
\end{equation}

For the training process, this discrepancy was resolved by simply dropping the last training sample, 
if the feature window belonging to it had missing bins.
For the prediction process, the feature array was padded with zeros on its end.
This procedure ensures that no errors are introduced into the \emph{training} process by imputing values,
but keeps the size of the \emph{predicted} matrix consistent with the training matrix binsizes.

\Cref{fig:methods:sample_gen_generalized} shows the generalized training process with a $(15\times15)$-training matrix and $k=2$. 
If $14\cdot b_{mat} + b_{feat} \geq cs > 14\cdot b_{mat} + 1$ holds for the chromosome size $cs$ in the example,
then the number of bins on the matrix diagonal will be $l_{mat}=15$, while the number of chromatin feature bins will be $l_{feat}=29 \not = 5 \cdot l_{mat}$.
\begin{figure}
 \begin{minipage}{0.60\textwidth}
   \centering
    \small
    \import{figures/}{sample_generation_process_generalized.pdf_tex}
    \caption{Generalized sample generation process f. $k=2$}
    \label{fig:methods:sample_gen_generalized}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin features array ($k=2$)
    \item sliding direction for sample generation windows
    \item first 4 diagonals of a $15\times15$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item first $4\times4$ training matrix, corrsponding to feature window A-B-C
\end{enumerate}
 \end{minipage}
\end{figure}
In this case, the 5th sample would be dropped for training,
while a column with zero bins -- symbolized by pink crosses in \cref{fig:methods:sample_gen_generalized} -- would be added for to the feature array for prediction
so that the resulting matrix would have the desired size of $15 \times 15$.




\subsection{Dense neural network approach}

\subsubsection{Basic setup} \label{sec:methods:basicSetup}
The basic setup for the dense neural network approach follows the proposal by Farr\'e et al. \cite{Farre2018a}.
It consists of five trainable layers; one convolutional layer and four densely connected layers, \cref{fig:methods:basic_dnn}.
For brevity, the sigmoid activation of the 1D convolution and the ReLU activations of the Dense layers are not shown.
The dense layers all use a ``L2'' kernel regularizer and the dropout layers following all but the last dense layer have a dropout probability of $10\%$.
\begin{figure}[htb]
    \small
    \centering
    \import{figures/}{DNN_basic_model.pdf_tex}
    \caption{Basic dense neural network}
    \label{fig:methods:basic_dnn}
\end{figure}

The training goal for the neural network $G$ is to find weights $\omega^*$ for its five trainable layers 
such that the mean squared error $L_2$ between the predicted submatrices $M_s = G_s(\omega)$ 
and the training submatrices $T_s$ becomes minimal for all $N$ training samples $s \in (1,2,\dots, N)$. 
Here, $M_s$ is given by the activations of the last dense layer, which are to be interpreted as the upper triangular 
part of a symmetric $(w\times w)$ Hi-C matrix.
Formally, one needs to optimize
\begin{equation}
 \omega^* = {arg\,min}_\omega L_2(\omega) = {arg\,min}_\omega \frac{1}{N} \sum_{s=0}^N (M_s - T_s)^2 \label{eq:methods:nn-mse}
\end{equation} 
For the thesis at hand, stochastic gradient with learning rate $1e^{-5}$  was used to find $\omega^*$.
Initial values $\omega_{init}$ were drawn from a Xavier initializer, a uniform distribution with limits depending on the in- and output shapes.
Following \cite{Farre2018a}, optimization was performed on minibatches of 30 samples, assembled randomly from the $N$ training samples
to prevent location-dependent effects and improve generalization.
For training, the last batch was dropped, if $N/30 \not \in \mathbb{N}$. 


\subsubsection{Modifying kernel size, number of filter layers and filters}
For the ``wider'' variant, the kernel size of the of the 1D convolutional layer was increased to $4k$ with strides $k$,
where $k=b_{mat}/b_{feat}$ is the relation beetween matrix- and feature binsize.
Mirror-type padding was used to maintain the output dimensions of the basic network, which now had \SI{7486478} trainable weights
for $k=1$.

For the ``longer'' variant three 1D-convolutional layers with 4, 8 and 16 filters 
were used in place of the basic network's single convolutional layer.
This replacement was also made for the ``wider-longer''-variant, 
additionally increasing the respective kernel sizes to $4k$ (with strides $k$), 4 and 4.
In both cases, the dropout rate was increased to 20\%.
The ``longer'' variant had \SI{9142665} trainable weights and the ``wider-longer''
had \xxx for $k=1$.



\subsubsection{Custom loss function based on TAD insulation score}
\begin{itemize}
 \item computation details, custom layer
 \item full insulation score takes too long, simplified
\end{itemize}

\subsubsection{Combination of mean squared error, perception loss and TV loss}
maybe not needed

\subsection{HiC-GAN approach}
\subsubsection{Using a DNN for 1D-2D conversion}
\subsubsection{Using a pre-trained DNN for 1D-2D conversion}
\subsubsection{Using a CNN for 1D-2D conversion}








