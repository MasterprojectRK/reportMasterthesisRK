\section{Methods}

\subsection{Input data}
For the thesis at hand, data from human cell lines GM12878, K562, HMEC, HUVEC and NHEK was used.
The exact data sources and data preprocessing will be outlined in the following subsections \ref{sec:methods:hicMatrices} and \ref{sec:methods:chipseq}.


\subsubsection{Hi-C matrices} \label{sec:methods:hicMatrices}
Hi-C data due to Rao et al. \cite{Rao2014} was downloaded 
in .hic format from Gene Expression Omnibus under accession key GSE63525.
Here, the quality-filtered ``combined\_30'' matrices were taken, which contain only high-quality reads from 
both replicates.

Next, matrices at \SI{5}{\kilo\bp} binsize were extracted and converted to cooler format using \texttt{hic2cool}
and subsequently coarsened to resolutions of 10, 25, 50 and \SI{100}{\kilo\bp} using \texttt{cooler coarsen}.

Contrary to the work from Farr\'e et al. \cite{Farre2018a}, which is using a distance-based normalization,
and many others in the field which are using ICE- or KR-normalization, 
these matrices have not been normalized for the thesis at hand
because no benefit of doing so was found during the study project \cite{Krauth2020}.

\subsubsection{ChIP-seq data} \label{sec:methods:chipseq}
For this thesis, ChIP-seq data for 13 chromatin features and DNaseI-seq data was used, cf.~\cref{tab:methods:csdata}.
Here, ChIP-seq data for the 13 features was downloaded in .bam format from the ENCODE project \xxx for both replicate one and two,
and DNaseI-seq data was downloaded in .bam format from \xxx; the download links are also given in \cref{tab:methods:csdata}.
\begin{table}[ht!]
\centering
 \begin{tabular}{ll}
 \hline
  feature name & download link \\  \hline
  CTCF & \\
  DNaseI &\\
  H3k27ac & \\
  H3k27me3 &\\
  H3k36me3 &\\
  H3k4me1 &\\
  H3k4me2 &\\
  H3k4me3 &\\
  H3k79me2 &\\
  H3k9ac &\\
  H3k9me3 &\\
  H4k20me1 &\\
  Rad21 &\\
  Smc3 & \\ \hline
 \end{tabular}
 \caption{chromatin features used for the thesis} \label{tab:methods:csdata}
\end{table}

The data were then converted to bigwig format, which is more convenient to handle, and the replicates were merged into 
one bigwig file. Pseudocode for the full conversion process is given in \xxx.

\subsubsection{Sample generation process for the dense neural network} \label{sec:methods:sample_gen}
This thesis follows the sliding window approach proposed by Farr\'e et al. \cite{Farre2018a}.

First, all chromatin features were binned to binsize $b_\mathit{feat}$ by splitting each chromosome of size $cs$ into 
$l_\mathit{feat}=\left \lceil{\frac{cs}{b_\mathit{feat}}}\right \rceil$ non-overlapping bins of size $b_\mathit{feat}$
and taking the mean feature value within the genomic region belonging to each bin.
All $n$ chromatin factors were processed in this way and then stacked into a $l_\mathit{feat} \times n$ array.

Hi-C matrices were taken as provided by the cooler format, cf. \cref{sec:methods:hicMatrices}, 
i.\,e. as $(l_\mathit{mat} \times l_\mathit{mat})$-matrices, $l_\mathit{mat}=\left \lceil{\frac{cs}{b_\mathit{mat}}}\right \rceil$ being the number of bins in the given chromosome. 
Initially, $b_\mathit{feat} = b_\mathit{mat}$ was used, which leads to $l_\mathit{feat} = l_\mathit{mat}$, because $cs$ is a constant for a given chromosome.

A sliding window approach was now applied to generate training samples for the neural networks $G$ described below.
Here, subarrays of size $(3w_\mathit{mat} \times n)$ were cut out from the feature array 
such that the $i$-th training sample corresponded to the subarray containing the columns $i,\,i+1,\,i+2,\,\dots,\,i+3w_\mathit{mat}$ of the full array. 
Sliding the window along the array with stepsize one obviously yields $N=l-3w_\mathit{mat}+1$ training samples.
The corresponding Hi-C matrices were then cut out along the diagonal of the original matrix 
as submatrices with corner indices [$j,\;j$], [$j,\;j+w_\mathit{mat}$], [$j+w_\mathit{mat},\;j+w_\mathit{mat}$], [$j+w_\mathit{mat},\;j$] in clockwise order, where $j=i+w_\mathit{mat}$.
The idea here is that the first $0,\,1,\,\dots \,w_\mathit{mat}$ columns of each feature sample form the left flanking region of the training matrix, 
the next $w_\mathit{mat}+1,\,w_\mathit{mat}+2,\,\dots \,2w_\mathit{mat}$ correspond to the matrix' region and the last $2w_\mathit{mat}+1,\,2w_\mathit{mat}+2,\,\dots \,3w_\mathit{mat}$ columns form the right flanking region.
Since Hi-C matrices are symmetric by definition, only the upper triangular part of the submatrices was used, 
flattened into a $w\cdot (w+1)/2$ vector.

\Cref{fig:methods:sample_gen} exemplarily shows the sample generation process for a $(16\times16)$-matrix with $w_\mathit{mat}=4$ and $n=3$ chromatin features.
In this case, five training samples would be generated -- the one encircled in green and four more to the right, as the window is sliding from left to right until the
right flanking region hits the feature array boundary.
\begin{figure}[hbp]
 \begin{minipage}{0.60\textwidth}
   \centering
    \small
    \import{figures/}{sample_generation_process.pdf_tex}
    \caption{Sample generation process}
    \label{fig:methods:sample_gen}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin feature array
    \item sliding direction for sample generation windows
    \item first 4 diagonals of a $16\times16$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item first $4\times4$training matrix, corrsponding to feature window A-B-C
\end{enumerate}
 \end{minipage}
\end{figure}

The sample generation process for predicting (unknown) matrices was the same as for training,
except that no matrix window was generated.
Due to the sliding window approach, the output of the network is a set of overlapping submatrices along the main diagonal of the actual target Hi-C matrix.
To generate the final submatrix, all submatrices were added up in a position-aware fashion 
and finally all values were divided by the number of overlaps for their respective position.
\Cref{fig:methods:prediction} exemplarily shows the prediction process for $N=5$ samples with windowsize $w=4$ for a $16\times16$ matrix.
Note that the first and last $w$ bins in each row (matrix diagonal) always remain empty due to the flanking regions,
as do all bins outside the main diagonal and the first $w-1$ side diagonals.
\begin{figure}
 \begin{minipage}{0.60\textwidth}
   \centering
    \small
    \import{figures/}{prediction_process2.pdf_tex}
    \caption{Prediction process}
    \label{fig:methods:prediction}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin feature array
    \item sliding direction for predicted submatrices
    \item first 4 diagonals of a predicted $16\times16$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item first predicted $4\times4$ matrix
\end{enumerate}
\raggedright{gray backround colors symbolize number of overlapping predictions at that position (bright = 1, dark = 4)}
 \end{minipage}
\end{figure}

The network and its learning algorithm was implemented in python using tensorflow deep learning framework with keras \xxx citation, see \xxx repository.
The basic network was used with parameters $n=14$ (cf.~\cref{sec:methods:chipseq}), $w=80$ and $b_\mathit{feat}=b_\mathit{mat} = \SI{25}{\kilo\bp}$. 

Training samples were drawn from GM12878, chromosomes 1, 2, 4, 7, 9, 11, 13, 14, 16, 17, 18, 20 and 22 of GM12878, 
validation samples from GM12878, chromosomes 6, 8, 12 and 15 and test samples from K562, chromosomes 3, 5, 10, 19 and 21,
see table \xxx for details.
Test samples were processed through the network and the resulting submatrices -- actually, vectors corresponding to their upper triangular parts -- were subsequently re-assembled into full Hi-C matrices.
Finally, the output matrices were scaled to value range [0...1000] for better visualisation and then stored in cooler format.

\subsubsection{Sample generation for the cGAN}
The samples for the cGAN were generated the same way as the samples for the dense neural network described 
in \cref{sec:methods:sample_gen}, with two exceptions.
First, Hi-C (sub-)matrices were not entered as vectors corresponding to their upper triangular part, 
but were instead taken as $w\times w \times 1$ grayscale images with value range [0...1] in 32-bit floating point format.
Second, for the cGAN approach the input matrices need not only be square,
but their sizes also needed to be powers of two. 
This ensured the required shapes for the connected up- and downsampling operations in the generator, 
which essentially are 2D-convolutions and transposed 2D-convolutions with strides two.
Within the thesis, windowsizes of $w=\{64,128,256\}$ were used, see \xxx.

\subsubsection{Generalization of feature binning} \label{sec:methods:inputBinning}
Most of the binning- and sample generation procedures described above 
also work for binsize relations $k=\frac{b_\mathit{mat}}{b_\mathit{feat}} \in \mathbb{N}^{>1}$.

The training matrices remain unchanged, i.\,e. $(l \times l)$-arrays, from which training submatrices of size  $w_\mathit{mat} \times w_\mathit{mat} $
can be extracted. 
With $k \in \mathbb{N}^{>1}$, one bin on the matrix diagonal corresponds to $k$ bins of the feature array,
so the feature windowsize needs to be $k$ times the submatrix windowsize, $w_\mathit{feat} = k \cdot w_\mathit{mat}$.
Since the first layer of all neural networks used in this thesis is a 1D convolution,
this can be achieved by setting the filter width and strides parameters of the (first) convolutional layer to $k$, leaving the rest of the network unchanged.
However, the number of bins along the matrix diagonal is generally not $k$ times the number of bins in the feature array,
see \cref{eq:methods:binning}.
\begin{equation}
 l_\mathit{feat} = \left \lceil{\frac{cs}{b_\mathit{feat}}}\right \rceil
                = \left \lceil{\frac{cs}{k \cdot b_\mathit{mat}}}\right \rceil 
                \not = k \cdot \left \lceil{\frac{cs}{ b_\mathit{mat}}}\right \rceil
                = k \cdot l_\mathit{mat} \label{eq:methods:binning}
\end{equation}

For the training process, this discrepancy was resolved by simply dropping the last training sample, 
if the feature window belonging to it had missing bins.
For the prediction process, the feature array was padded with zeros on its end.
This procedure ensures that no errors are introduced into the \emph{training} process by imputing values,
but keeps the size of the \emph{predicted} matrix consistent with the training matrix binsizes.

\Cref{fig:methods:sample_gen_generalized} shows the generalized training process with a $(16\times16)$-training matrix and $k=2$. 
If  $15\cdot b_\mathit{mat} + 1 \leq cs < 15\cdot b_\mathit{mat} + b_\mathit{feat}$ held for the chromosome size $cs$ in the example,
then the number of bins on the matrix diagonal would be $l_\mathit{mat}=16$, while the number of chromatin feature bins would be $l_\mathit{feat}=31 \not = 2 \cdot l_\mathit{mat}$.
\begin{figure}
 \begin{minipage}{0.60\textwidth}
   \centering
    \small
    \import{figures/}{sample_generation_process_generalized.pdf_tex}
    \caption{Generalized sample generation process f. $k=2$}
    \label{fig:methods:sample_gen_generalized}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin features array ($k=2$)
    \item sliding direction for sample generation windows
    \item first 4 diagonals of a $16\times16$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item last $4\times4$ training matrix, corrsponding to feature window A-B-C; to be dropped/padded
\end{enumerate}
 \end{minipage}
\end{figure}
In this case, the 5th sample would be dropped for training,
while a column with zero bins -- symbolized by pink crosses in \cref{fig:methods:sample_gen_generalized} -- would be added to the feature array for prediction
so that the resulting matrix would still have the desired size of $16 \times 16$.

\subsection{Dense neural network approach}

\subsubsection{Basic setup} \label{sec:methods:basicSetup}
The basic setup for the dense neural network approach follows the proposal by Farr\'e et al. \cite{Farre2018a}.
It consists of five trainable layers; one convolutional layer and four densely connected layers with 7,486,436 trainable weights in total, \cref{fig:methods:basic_dnn}.
For brevity, the sigmoid activation of the 1D convolution and the ReLU activations of the Dense layers are not shown.
The dense layers all use a ``L2'' kernel regularizer and the dropout layers following all but the last dense layer have a dropout probability of $10\%$.
\begin{figure}[htb]
    \small
    \centering
    \import{figures/}{DNN_basic_model.pdf_tex}
    \caption{Basic dense neural network}
    \label{fig:methods:basic_dnn}
\end{figure}

The training goal for the neural network $G$ is to find weights $\omega^*$ for its five trainable layers 
such that the mean squared error $L_2$ between the predicted submatrices $M_s = G_s(\omega)$ 
and the training submatrices $T_s$ becomes minimal for all $N$ training samples $s \in (1,2,\dots, N)$. 
Here, $M_s$ is given by the activations of the last dense layer, which are to be interpreted as the upper triangular 
part of a symmetric $(w\times w)$ Hi-C matrix.
Formally, one needs to optimize
\begin{equation}
 \omega^* = \mathit{arg\,min}_\omega L_2(\omega) = \mathit{arg\,min}_\omega \frac{1}{N} \sum_{s=0}^N (M_s - T_s)^2 \label{eq:methods:nn-mse}
\end{equation} 
For the thesis at hand, stochastic gradient with learning rate $1e^{-5}$  was used to find $\omega^*$.
Initial values $\omega_\mathit{init}$ were drawn from a Xavier initializer, a uniform distribution with limits depending on the in- and output shapes.
Following \cite{Farre2018a}, optimization was performed on minibatches of 30 samples, assembled randomly from the $N$ training samples
to prevent location-dependent effects and improve generalization.
For training, the last batch was dropped, if $N/30 \not \in \mathbb{N}$. 


\subsubsection{Modifying kernel size, number of filter layers and filters} \label{sec:methods:variants}
For the ``wider'' variant, the kernel size of the of the 1D convolutional layer was increased to $4k$ with strides $k$,
where $k=b_\mathit{mat}/b_\mathit{feat}$ is the relation beetween matrix- and feature binsize.
Mirror-type padding was used to maintain the output dimensions of the basic network, which now had \SI{7486478} trainable weights
for $k=1$.

For the ``longer'' variant three 1D-convolutional layers with 4, 8 and 16 filters 
were used in place of the basic network's single convolutional layer.
This replacement was also made for the ``wider-longer''-variant, 
additionally increasing the respective kernel sizes to $4k$ (with strides $k$), 4 and 4.
In both cases, the dropout rate was increased to 20\%.
The ``longer'' variant had \SI{9142665} trainable weights and the ``wider-longer''
had \SI{9143313} for $k=1$.

For the variant with generalized feature binning, features were binned at $b_\textrm{feat} = \SI{5}{\kilo\bp}$ while keeping the matrix binsize $b_\textrm{mat} = \SI{25}{\kilo\bp}$,
so the factor for the relation between the two binsizes was $k=25/5=5$.
This yields input feature arrays of size $3w\cdot k \times n = 3\cdot80\cdot5 \times 14 = 1200\times 14$.
Replacing the first convolutional layer of the basic network by a 1D convolutional filter with kernel size $k=5$ and strides $k=5$,
this input was again compressed to a $3w\times 1 = 240\times 1$ tensor and fed into the flatten layer, cf~\cref{fig:methods:basic_dnn}. 
The rest of the network remained the same so that the number of trainable parameters increased only slightly to \SI{7486492}.

\subsubsection{Custom loss function based on TAD insulation score}
\begin{itemize}
 \item computation details, custom layer
 \item full insulation score takes too long, simplified
\end{itemize}

\subsubsection{Combination of mean squared error, perception loss and TV loss} \label{sec:methods:combined_loss}
For computing the combined MSE, perception- and TV-loss,
input features were first passed through the network as normal,
and mean squared error was computed on the predicted ``upper triangular vectors'' vs. the real vectors.
Next, both the output- and target vectors were converted to symmetric grayscale images by embedding them into 
$w \times w \times 1$ tensors, where $w$ is again the windowsize.
TV loss was then computed by simply using the appropriate tensorflow function on the predicted images.
For perception loss, the predicted images and the true images were first fed through a pre-trained VGG16 network truncated at layer \xxx.
The loss was then computed as mean squared error between the ``predicted'' and ``true'' activations at the output of that network.

Let $M_s=G_s(\omega)$ again be the output of the neural network $G$ described above, and $T_s$ the true matrices for training samples $s$ in vector form,
and let $M^\prime_s$ and $T^\prime_s$ be their grayscale image counterparts as described above.
Furthermore, let $\mathit{tv}(x)$ be the total variation of image $x$ and $\mathit{vgg}(x)$ the output of the perception loss network for image $x$.
The goal of the modified network was now to find weights $w^*$ such that
\begin{equation}
 \omega^* = \mathit{arg\,min}_\omega (  \lambda_\mathit{MSE} \frac{1}{N} \sum_{s=0}^N (M_s - T_s)^2 
                                                     + \lambda_\mathit{TV} \sum_{s=0}^N \mathit{tv}( M^\prime_s) 
                                                     + \lambda_\mathit{VGG} \frac{1}{N} \sum_{s=0}^N (\mathit{vgg}(M^\prime_s) - \mathit{vgg}(T^\prime_s))^2 ) \label{eq:methods:combined_loss}
\end{equation}
Weight initialization for network $G$ and minibatching was done as described in \cref{sec:methods:basicSetup}.
The weights for the VGG16 network were taken from the pre-trained keras implementation and can here be considered as constants.
In addition, the usage of VGG16 imposes the restriction $w \geq 32$ on the windowsize $w$, which is not problematic, since again $w=80$ was chosen for all experiments.

The network $G(\omega)$ could in principle be any of the variants described above in \cref{sec:methods:variants},
but for the thesis at hand, only the initial network from \cref{sec:methods:basicSetup} was used.

\subsubsection{Combination of mean squared error and TAD-score-based loss} \label{sec:methods:score_loss}
Formally, the combination between mean squared error and TAD-score-based loss can be defined as the following optimization task:
\begin{equation}
 \omega^* = \mathit{arg\,min}_\omega (  \lambda_\mathit{MSE} \frac{1}{N} \sum_{s=0}^N (M_s - T_s)^2
                                                    + \lambda_\mathit{score} \frac{1}{N} \sum_{s=0}^N (\mathit{score}(M^\prime_s,ds) - \mathit{score}(T^\prime_s,ds))^2 \label{eq:methods:score_loss}
\end{equation}
where $M_s$ is again the Hi-C submatrix predicted by the network $G(\omega)$ for sample $s$ and $T_s$ is the corresponding true Hi-C submatrix.

To compute the TAD-score-based loss $\mathit{score}(\cdot,\cdot)$, the predictions and true Hi-C matrices $M$ and $T$ in vector form (upper triangular part)
were first converted back to complete, symmetric Hi-C matrices $M^\prime,\; T^\prime$. 
Next, in a custom network layer, all diamonds with size $ds$ inside the submatrices of size $w$ were cut out using tensor slicing and the values inside the diamonds were reduced to their respective mean.
This yields score vectors -- more exactly, tensors with shape $(w - 2*ds, 1)$.
After computing the latter for both predicted- and real Hi-C submatrices, the mean squared error between them was computed as usual and weighted with 
a user-selected loss weight $\lambda_\mathit{score}$, see \cref{eq:methods:score_loss}.

While it would also have been possible, and probably faster, to slice the outputs of the networks directly in vector form, 
this is rather unintuitive and was therefore not done for the thesis. 

\subsection{HiC-GAN approach}
\subsubsection{Pix2Pix network layout}
\subsubsection{Using a DNN for 1D-2D conversion}
\subsubsection{Using a pre-trained DNN for 1D-2D conversion}
\subsubsection{Using a CNN for 1D-2D conversion}








