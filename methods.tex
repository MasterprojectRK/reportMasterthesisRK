\section{Methods}

\subsection{Input data}
For the thesis at hand, data from human cell lines GM12878, K562, HMEC, HUVEC and NHEK was used.
The exact data sources and data preprocessing will be outlined in the following subsections \ref{sec:methods:hicMatrices} and \ref{sec:methods:chipseq}.

\subsubsection{Hi-C matrices} \label{sec:methods:hicMatrices}
Hi-C data due to Rao et al. \cite{Rao2014} was downloaded 
in .hic format from Gene Expression Omnibus under accession key GSE63525.
Here, the quality-filtered ``combined\_30'' matrices were taken, which contain only high-quality reads from 
both replicates.

Next, matrices at \SI{5}{\kilo\bp} binsize were extracted and converted to cooler format using \texttt{hic2cool}
and subsequently coarsened to resolutions of 10, 25, 50 and \SI{100}{\kilo\bp} using \texttt{cooler coarsen}.

Contrary to the work from Farr\'e et al. \cite{Farre2018a}, which is using a distance-based normalization,
and many others in the field which are using ICE- or KR-normalization, 
these matrices have not been normalized for the thesis at hand
because no benefit of doing so was found during the study project \cite{Krauth2020}.

\subsubsection{ChIP-seq data} \label{sec:methods:chipseq}
For this thesis, ChIP-seq data for 13 chromatin features and DNaseI-seq data was used.
Here, the data was downloaded in .bam format either via the ENCODE project \cite{Encode2012,Davis2017} 
or directly from the file server of the University of California. 
In all cases, bam-files for replicate 1 and 2 were downloaded in their most recent versions (if applicable);
the UCSC- (wgEncode...) or GEO- (GSM...) identifiers are given in \cref{tab:methods:csdata}.
For convenience, the pdf version of this document also provides download links for concrete files in \cref{sec:chromFeat_download_links}.
\begin{table}[ht!]
\centering
 \begin{tabular}{lll}
 \hline
  feature name & GM12878 & K562 \\  \hline
  CTCF & GSM733752 & GSM733719\\
  DNaseI & wgEncodeEH000534 & wgEncodeEH000530\\
  H3k27ac & GSM733771 & GSM733656\\
  H3k27me3 &GSM733758 & GSM733658\\
  H3k36me3 &GSM733679 & GSM733714\\
  H3k4me1  &GSM733772 & GSM733692\\
  H3k4me2  &GSM733769 & GSM733651\\
  H3k4me3 &GSM733708 & GSM733680\\
  H3k79me2 &GSM733736	& GSM733653	\\
  H3k9ac &GSM733677 & GSM733778\\
  H3k9me3 &GSM733664 & GSM733776	\\
  H4k20me1 &GSM733642 & GSM733675\\
  Rad21 &	wgEncodeEH000749 & wgEncodeEH000649\\
  Smc3 & 	wgEncodeEH001833 & wgEncodeEH001845\\ \hline
 \end{tabular}
 \caption{chromatin features used for the thesis} \label{tab:methods:csdata}
\end{table}

After downloading, the bam files were converted to bigwig format, which was found more convenient to handle, and the replicates were merged into 
one bigwig file by taking the mean, as in the study project \cite{Krauth2020}. Pseudocode for the full conversion process is given in \xxx.

The choice of chromatin features is widely in line with the work by Zhang et al. \cite{Zhang2019}; it contains
structural proteins like CTCF and Cohesin subcomponents RAD21 and SMC3 as well as active/passive markers.

\subsubsection{Sample generation process for the dense neural network} \label{sec:methods:sample_gen}
This thesis follows the sliding window approach proposed by Farr\'e et al. \cite{Farre2018a}.

First, all chromatin features were binned to binsize $b_\mathit{feat}$ by splitting each chromosome of size $cs$ into 
$l_\mathit{feat}=\left \lceil{\frac{cs}{b_\mathit{feat}}}\right \rceil$ non-overlapping bins of size $b_\mathit{feat}$
and taking the mean feature value within the genomic region belonging to each bin.
All $n$ chromatin factors were processed in this way and then stacked into a $l_\mathit{feat} \times n$ array.
Here, the number of chromatin features was constant for all investigations within this thesis, $n=14$ (cf.~\cref{sec:methods:chipseq}). 

Separate Hi-C matrices for each chromosome were derived from the cooler format as $(l_\mathit{mat} \times l_\mathit{mat})$-matrices, 
$l_\mathit{mat}=\left \lceil{\frac{cs}{b_\mathit{mat}}}\right \rceil$ being the number of bins in the given chromosome. 
Initially, $b_\mathit{feat} = b_\mathit{mat}$ was used, which leads to $l_\mathit{feat} = l_\mathit{mat}$, because $cs$ is a constant for a given chromosome.

A sliding window approach was now applied to generate training samples for the neural networks $G$ described below.
Here, subarrays of size $(3w_\mathit{mat} \times n)$ were cut out from the feature array 
such that the $i$-th training sample corresponded to the subarray containing the columns $i,\,i+1,\,i+2,\,\dots,\,i+3w_\mathit{mat}$ of the full array. 
Sliding the window along the array with stepsize one obviously yields $N=l-3w_\mathit{mat}+1$ training samples.
The corresponding Hi-C matrices were then cut out along the diagonal of the original matrix 
as submatrices with corner indices [$j,\;j$], [$j,\;j+w_\mathit{mat}$], [$j+w_\mathit{mat},\;j+w_\mathit{mat}$], [$j+w_\mathit{mat},\;j$] in clockwise order, where $j=i+w_\mathit{mat}$.
The idea here is that the first $0,\,1,\,\dots \,w_\mathit{mat}$ columns of each feature sample form the left flanking region of the training matrix, 
the next $w_\mathit{mat}+1,\,w_\mathit{mat}+2,\,\dots \,2w_\mathit{mat}$ correspond to the matrix' region and the last $2w_\mathit{mat}+1,\,2w_\mathit{mat}+2,\,\dots \,3w_\mathit{mat}$ columns form the right flanking region.
Since Hi-C matrices are symmetric by definition, only the upper triangular part of the submatrices was used, 
flattened into a $w\cdot (w+1)/2$ vector.

\Cref{fig:methods:sample_gen} exemplarily shows the sample generation process for a $(16\times16)$-matrix with $w_\mathit{mat}=4$ and $n=3$ chromatin features.
In this case, five training samples would be generated -- the one encircled in green and four more to the right, as the window is sliding from left to right until the
right flanking region hits the feature array boundary.
\begin{figure}[hbp]
 \begin{minipage}{0.65\textwidth}
   \resizebox{\textwidth}{!}{
    \small
    \import{figures/}{sample_generation_process.pdf_tex}}
    \caption{Sample generation process}
    \label{fig:methods:sample_gen}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin feature array
    \item sliding direction for sample generation windows
    \item first 4 diagonals of a $16\times16$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item first $4\times4$training matrix, corrsponding to feature window A-B-C
\end{enumerate}
 \end{minipage}
\end{figure}

The sample generation process for predicting (unknown) matrices was the same as for training,
except that no matrix window was generated.
Due to the sliding window approach, the output of the network is a set of overlapping submatrices along the main diagonal of the actual target Hi-C matrix.
To generate the final submatrix, all submatrices were added up in a position-aware fashion 
and finally all values were divided by the number of overlaps for their respective position.
\Cref{fig:methods:prediction} exemplarily shows the prediction process for $N=5$ samples with windowsize $w=4$ for a $16\times16$ matrix.
Note that the first and last $w$ bins in each row (matrix diagonal) always remain empty due to the flanking regions,
as do all bins outside the main diagonal and the first $w-1$ side diagonals.
To improve visualisation, all predicted matrices were scaled to value range 0...1000 after re-assembly and stored in cooler format for further processing.
Conveniently, cooler also supports storing only the upper triangular part of symmetric matrices, minimizing conversion effort for the data at hand.

\begin{figure}
 \begin{minipage}{0.65\textwidth}
   \resizebox{\textwidth}{!}{
    \small
    \import{figures/}{prediction_process2.pdf_tex}}
    \caption{Prediction process}
    \label{fig:methods:prediction}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin feature array
    \item sliding direction for predicted submatrices
    \item first 4 diagonals of a predicted $16\times16$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item first predicted $4\times4$ matrix
\end{enumerate}
\raggedright{gray backround colors symbolize number of overlapping predictions at that position (bright = 1, dark = 4)}
 \end{minipage}
\end{figure}

Training samples were drawn from GM12878, chromosomes 1, 2, 4, 7, 9, 11, 13, 14, 16, 17, 18, 20 and 22 of GM12878, 
validation samples from GM12878, chromosomes 6, 8, 12 and 15 and test samples from K562, chromosomes 3, 5, 10, 19 and 21.
This approximately implements a 60:20:20 train:validation:test split, see \cref{tab:methods:samples} for details.
Note that windowsize $w=80$ is not suitable for resolutions beyond \SI{50}{\kilo\bp}, because the number of training samples becomes too small
to train a network with more than seven million parameters, cf.~\cref{sec:methods:basicSetup}.
\begin{table}[hbp]
\centering
\begin{tabular}{lrrrrrrr}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{chrom.}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{length/bp}}} & \multicolumn{6}{c}{\textbf{samples at $\mathbf{w=80}$ and binsize...}}                                                                                                \\
\multicolumn{1}{c}{}                                     & \multicolumn{1}{c}{}                                    & \multicolumn{1}{c}{5k} & \multicolumn{1}{c}{10k} & \multicolumn{1}{c}{25k} & \multicolumn{1}{c}{50k} & \multicolumn{1}{c}{250k} & \multicolumn{1}{c}{500k} \\ \hline
1                                                        & 249250621                                               & 49612                  & 24687                   & 9732                    & 4747                    & 759                      & 260                      \\
2                                                        & 243199373                                               & 48401                  & 24081                   & 9489                    & 4625                    & 734                      & 248                      \\
4                                                        & 191154276                                               & 37992                  & 18877                   & 7408                    & 3585                    & 526                      & 144                      \\
7                                                        & 159138663                                               & 31589                  & 15675                   & 6127                    & 2944                    & 398                      & 80                       \\
9                                                        & 141213431                                               & 28004                  & 13883                   & 5410                    & 2586                    & 326                      & 44                       \\
11                                                       & 135006516                                               & 26763                  & 13262                   & 5162                    & 2462                    & 302                      & 32                       \\
13                                                       & 115169878                                               & 22795                  & 11278                   & 4368                    & 2065                    & 222                      &                          \\
14                                                       & 107349540                                               & 21231                  & 10496                   & 4055                    & 1908                    & 191                      &                          \\
16                                                       & 90354753                                                & 17832                  & 8797                    & 3376                    & 1569                    & 123                      &                          \\
17                                                       & 81195210                                                & 16001                  & 7881                    & 3009                    & 1385                    & 86                       &                          \\
18                                                       & 78077248                                                & 15377                  & 7569                    & 2885                    & 1323                    & 74                       &                          \\
20                                                       & 63025520                                                & 12367                  & 6064                    & 2283                    & 1022                    & 14                       &                          \\
22                                                       & 51304566                                                & 10022                  & 4892                    & 1814                    & 788                     &                          &                          \\
\multicolumn{2}{r}{$\mathbf{\sum}$ \textbf{train samples}}                          & \textbf{337986} & \textbf{167442}  & \textbf{65118}   & \textbf{31009}   & \textbf{3755}   & \textbf{808}                      \\
                                                         &                                                         &                        &                         &                         &                         &                          &                          \\
6                                                        & 171115067                                               & 33985                  & 16873                   & 6606                    & 3184                    & 446                      & 104                      \\
8                                                        & 146364022                                               & 29034                  & 14398                   & 5616                    & 2689                    & 347                      & 54                       \\
12                                                       & 133851895                                               & 26532                  & 13147                   & 5116                    & 2439                    & 297                      & 29                       \\
15                                                       & 102531392                                               & 20268                  & 10015                   & 3863                    & 1812                    & 172                      &                          \\
\multicolumn{2}{r}{$\mathbf{\sum}$ \textbf{valid. samples}}                  & \textbf{109819}    & \textbf{54433}       & \textbf{21201}    & \textbf{10124}     & \textbf{1262}         & \textbf{187}    \\
                                                         &                                                         &                        &                         &                         &                         &                          &                          \\
3                                                        & 198022430                                               & 39366                  & 19564                   & 7682                    & 3722                    & 554                      & 158                      \\
5                                                        & 180915260                                               & 35945                  & 17853                   & 6998                    & 3380                    & 485                      & 123                      \\
10                                                       & 135534747                                               & 26868                  & 13315                   & 5183                    & 2472                    & 304                      & 33                       \\
19                                                       & 59128983                                                & 11587                  & 5674                    & 2127                    & 944                     &                          &                          \\
21                                                       & 48129895                                                & 9387                   & 4574                    & 1687                    & 724                     &                          &                          \\
\multicolumn{2}{r}{$\mathbf{\sum}$ \textbf{test samples}}                                                                          & \textbf{123153}        & \textbf{60980}          & \textbf{23677}          & \textbf{11242}          & \textbf{1343}            & \textbf{314}             \\
                                                         &                                                         & \multicolumn{1}{l}{}   & \multicolumn{1}{l}{}    & \multicolumn{1}{l}{}    & \multicolumn{1}{l}{}    & \multicolumn{1}{l}{}     & \multicolumn{1}{l}{}     \\
\multicolumn{2}{r}{$\mathbf{\sum}$ \textbf{total samples}}                                                                         & \textbf{570958}        & \textbf{282855}         & \textbf{109996}         & \textbf{52375}          & \textbf{6360}            & \textbf{1309}            \\ \hline
\end{tabular}
\caption{training, validation and test samples for sliding window approach} \label{tab:methods:samples}
\end{table}

\subsubsection{Sample generation for the cGAN} \label{sec:methods:sample_gen_cgan}
The samples for the cGAN were generated the same way as the samples for the dense neural network described 
in \cref{sec:methods:sample_gen}, with two exceptions.
First, Hi-C (sub-)matrices were not entered as vectors corresponding to their upper triangular part, 
but were instead taken as $w\times w \times 1$ grayscale images with value range [0...1] in 32-bit floating point format.
Second, for the cGAN approach the input matrices need not only be square,
but their sizes also needed to be powers of two. 
This ensured the required shapes for the connected up- and downsampling operations in the generator, 
which essentially are 2D-convolutions and transposed 2D-convolutions with strides two.
Within the thesis, windowsizes of $w=\{64,128,256\}$ were used, see \cref{sec:results:cgan}.

\subsubsection{Generalization of feature binning} \label{sec:methods:inputBinning}
Most of the binning- and sample generation procedures described above 
also work for binsize relations $k=\frac{b_\mathit{mat}}{b_\mathit{feat}} \in \mathbb{N}^{>1}$.

The training matrices remain unchanged, i.\,e. $(l \times l)$-arrays, from which training submatrices of size  $w_\mathit{mat} \times w_\mathit{mat} $
can be extracted. 
With $k \in \mathbb{N}^{>1}$, one bin on the matrix diagonal corresponds to $k$ bins of the feature array,
so the feature windowsize needs to be $k$ times the submatrix windowsize, $w_\mathit{feat} = k \cdot w_\mathit{mat}$.
Since the first layer of all neural networks used in this thesis is a 1D convolution,
this can be achieved by setting the filter width and strides parameters of the (first) convolutional layer to $k$, leaving the rest of the network unchanged.
However, the number of bins along the matrix diagonal is generally not $k$ times the number of bins in the feature array,
see \cref{eq:methods:binning}.
\begin{equation}
 l_\mathit{feat} = \left \lceil{\frac{cs}{b_\mathit{feat}}}\right \rceil
                = \left \lceil{\frac{cs}{k \cdot b_\mathit{mat}}}\right \rceil 
                \not = k \cdot \left \lceil{\frac{cs}{ b_\mathit{mat}}}\right \rceil
                = k \cdot l_\mathit{mat} \label{eq:methods:binning}
\end{equation}

For the training process, this discrepancy was resolved by simply dropping the last training sample, 
if the feature window belonging to it had missing bins.
For the prediction process, the feature array was padded with zeros on its end.
This procedure ensures that no errors are introduced into the \emph{training} process by imputing values,
but keeps the size of the \emph{predicted} matrix consistent with the training matrix binsizes.

\Cref{fig:methods:sample_gen_generalized} shows the generalized training process with a $(16\times16)$-training matrix and $k=2$. 
If  $15\cdot b_\mathit{mat} + 1 \leq cs < 15\cdot b_\mathit{mat} + b_\mathit{feat}$ held for the chromosome size $cs$ in the example,
then the number of bins on the matrix diagonal would be $l_\mathit{mat}=16$, while the number of chromatin feature bins would be $l_\mathit{feat}=31 \not = 2 \cdot l_\mathit{mat}$.
\begin{figure}
 \begin{minipage}{0.65\textwidth}
   \resizebox{\textwidth}{!}{
    \small
    \import{figures/}{sample_generation_process_generalized.pdf_tex}}
    \caption{Generalized sample generation process f. $k=2$}
    \label{fig:methods:sample_gen_generalized}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin features array ($k=2$)
    \item sliding direction for sample generation windows
    \item first 4 diagonals of a $16\times16$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item last $4\times4$ training matrix, corrsponding to feature window A-B-C; to be dropped/padded
\end{enumerate}
 \end{minipage}
\end{figure}
In this case, the 5th sample would be dropped for training,
while a column with zero bins -- symbolized by pink crosses in \cref{fig:methods:sample_gen_generalized} -- would be added to the feature array for prediction
so that the resulting matrix would still have the desired size of $16 \times 16$.

\subsection{Quality metrics for predicted Hi-C matrices}
Assessing the quality of synthetically generated images is a long-standing problem, which has not yet been solved, see e\,g. \cite[p.\,19]{Wang2020}.
Within this thesis, distance-stratified Pearson correlations were computed between predicted and real matrices to measure the quality of the predictions.
While the suitability of suchlike correlations as a quality metric for Hi-C matrices seems debatable in general \cite{Yang2017}, 
distance stratified Pearson correlation is quite common in the field of Hi-C matrices and thus useful at least for comparisons with other approaches.

To compute the correlations, the $w$ bins at the left and right boundaries of the investigated matrices were first removed, because these remain zero 
in the predictions due to the chosen sample generation approach, 
cf.~\cref{sec:methods:sample_gen}, \cref{fig:methods:prediction} and \ref{sec:methods:sample_gen_cgan}.
Next, the remaining values were grouped according to their genomic distance, up to the maximum distance $w$, 
and then Pearson correlations were computed as usual, separately for each group.
Additionally, the area under the correlation curve (AUC) was computed for all curves in each plot to obtain a single-valued, albeit very abstract quality metric.
Furthermore, all Pearson correlation plots show the correlation between the ``true'' GM12878 matrix and the corresponding target matrix as a baseline,
i.\,e. the Pearson correlation that is obtained when simply using the relevant GM12878 matrix as a ``prediction'' for a given target matrix.

Since relying on one metric alone seemed problematic, pygenometracks \cite{LopezDelisle2020} was used to plot selected areas of the test set 
where the characteristics of the results seemed particularly distinct.

In computer graphics, other metrics like the structural similarity index (SSIM), peak signal-to-noise ratio (PSNR) or, especially for images generated by GANs, 
Frechet inception score (FIS) are more common.
However, all of these operate on images, and no useful images seem available for comparisons within this thesis.
Images of complete Hi-C matrices, i.\,e. for full chromosomes,
can be generated from cooler matrices, but contain only zeros outside the first $w$ diagonals.
This would lead to false results when compared to the true matrices, whether they are truncated at $w$ or not.
Images of $(w\times w)$ sub-matrices, on the other hand, are generated natively, but are overlapping and thus highly correlated among each other due to the sliding window approach for sample generation, 
cf. \cref{sec:methods:sample_gen} and \ref{sec:methods:sample_gen_cgan}. 
Again, the result would be not very meaningful, or at least difficult to interpret.

\subsection{Dense neural network approach}

\subsubsection{Basic setup} \label{sec:methods:basicSetup}
The basic setup for the dense neural network approach closely follows the proposal by Farr\'e et al. \cite{Farre2018a},
so a windowsize of $w=80$ and $b_\mathit{feat}=b_\mathit{mat} = \SI{10}{\kilo\bp}$ was initially used.
With these parameters, the network has a total of 7,486,436 trainable weights in five trainable layers; 
one convolutional layer and four densely connected layers, \cref{fig:methods:basic_dnn}.
For brevity, the sigmoid activation after the 1D convolution and the ReLU activations after the Dense layers are not shown.
The dense layers all use a ``L2'' kernel regularizer and the dropout layers following all but the last dense layer have a dropout probability of $10\%$.
\begin{figure}[htb]
    \small
    \centering
    \import{figures/}{DNN_basic_model.pdf_tex}
    \caption{Basic dense neural network}
    \label{fig:methods:basic_dnn}
\end{figure}

The training goal for the neural network $G$ is to find weights $\omega^*$ for its five trainable layers 
such that the mean squared error $L_2$ between the predicted submatrices $M_s = G_s(\omega)$ 
and the training submatrices $T_s$ becomes minimal for all $N$ training samples $s \in (1,2,\dots, N)$. 
Here, $M_s$ is given by the activations of the last dense layer, which are to be interpreted as the upper triangular 
part of a symmetric $(w\times w)$ Hi-C matrix.
Formally, one needs to optimize
\begin{equation}
 \omega^* = \mathit{arg\,min}_\omega L_2(\omega) = \mathit{arg\,min}_\omega \frac{1}{N} \sum_{s=0}^N (M_s - T_s)^2 \label{eq:methods:nn-mse}
\end{equation} 
For the thesis at hand, stochastic gradient descent (SGD) with learning rate $10^{-5}$  was used to find $\omega^*$.
Initial values $\omega_\mathit{init}$ were drawn from a Xavier initializer, a uniform distribution with limits depending on the in- and output shapes.
Following \cite{Farre2018a}, optimization was performed on minibatches of 30 samples, assembled randomly from the $N$ training samples
to prevent location-dependent effects and improve generalization.
For training, the last batch was dropped, if $N/30 \not \in \mathbb{N}$. 

The network and its learning algorithm were implemented in python using tensorflow deep learning framework, partially with keras frontend \cite{Abadi2015,Chollet2015}, 
see repository \xxx.

\subsubsection{Modifying kernel size, number of filter layers and filters} \label{sec:methods:variants}
For the ``wider'' variant, the kernel size of the of the 1D convolutional layer was increased to $4k$ with strides $k$,
where $k=b_\mathit{mat}/b_\mathit{feat}$ is the relation beetween matrix- and feature binsize.
Mirror-type padding was used to maintain the output dimensions of the basic network, which now had \SI{7486478} trainable weights
for $k=1$.

For the ``longer'' variant three 1D-convolutional layers with 4, 8 and 16 filters 
were used in place of the basic network's single convolutional layer.
This replacement was also made for the ``wider-longer''-variant, 
additionally increasing the respective kernel sizes to $4k$ (with strides $k$), 4 and 4.
In both cases, the dropout rate was increased to 20\%.
The ``longer'' variant had \SI{9142665} trainable weights and the ``wider-longer''
had \SI{9143313} for $k=1$.

For the variant with generalized feature binning, features were binned at $b_\textrm{feat} = \SI{5}{\kilo\bp}$ while keeping the matrix binsize $b_\textrm{mat} = \SI{25}{\kilo\bp}$,
so the factor for the relation between the two binsizes was $k=25/5=5$.
This yields input feature arrays of size $3w\cdot k \times n = 3\cdot80\cdot5 \times 14 = 1200\times 14$.
Replacing the first convolutional layer of the basic network by a 1D convolutional filter with kernel size $k=5$ and strides $k=5$,
this input was again compressed to a $3w\times 1 = 240\times 1$ tensor and fed into the flatten layer, cf~\cref{fig:methods:basic_dnn}. 
The rest of the network remained the same so that the number of trainable parameters increased only slightly to \SI{7486492}.

\subsubsection{Combination of mean squared error, perception loss and TV loss} \label{sec:methods:combined_loss}
For computing the combined MSE, perception- and TV-loss,
input features were first passed through the network as normal,
and mean squared error was computed on the predicted ``upper triangular vectors'' vs. the real vectors.
Next, both the output- and target vectors were converted to symmetric grayscale images by embedding them into 
$w \times w \times 1$ tensors, where $w$ is again the windowsize.

For an image, total variation can be defined as the sum of the absolute differences for neighboring pixel-values in an image, \cref{eq:methods:tv} \cite{Rudin1992}
\begin{equation}
 \sum_{i,j}\sqrt{|y_{i+1,j} - y_{i,j}|^2 + |y_{i,j+1} - y_{i,j} |^2 } \label{eq:methods:tv}
\end{equation}
For simplicity the tensorflow implementation from tf.image.total\_variation was used,
taking the \xxx sum \xxx mean across batches, as recommended in the tensorflow documentation.

For perception loss, the predicted images and the true images were first fed through a pre-trained VGG-16 network with fixed weights, truncated at layer \xxx.
The loss was then computed as mean squared error between the ``predicted'' and ``true'' output activations of the truncated VGG-16 network.

Let $M_s=G_s(\omega)$ again be the output of the neural network $G$ described above, and $T_s$ the true matrices for training samples $s$ in vector form,
and let $M^\prime_s$ and $T^\prime_s$ be their grayscale image counterparts as described above.
Furthermore, let $\mathit{tv}(x)$ be the total variation of image $x$ and $\mathit{vgg}(x)$ the output of the perception loss network for image $x$.
The goal of the modified network was now to find weights $w^*$ such that
\begin{equation}
 \omega^* = \mathit{arg\,min}_\omega (  \lambda_\mathit{MSE} \frac{1}{N} \sum_{s=0}^N (M_s - T_s)^2 
                                                     + \lambda_\mathit{TV} \sum_{s=0}^N \mathit{tv}( M^\prime_s) 
                                                     + \lambda_\mathit{VGG} \frac{1}{N} \sum_{s=0}^N (\mathit{vgg}(M^\prime_s) - \mathit{vgg}(T^\prime_s))^2 ) \label{eq:methods:combined_loss}
\end{equation}
Weight initialization for network $G$ and minibatching was done as described in \cref{sec:methods:basicSetup}.
The weights for the VGG16 network were taken from the pre-trained keras implementation and can here be considered as constants.
In addition, the usage of VGG16 imposes the restriction $w \geq 32$ on the windowsize $w$, which is not problematic, since again $w=80$ was chosen for all experiments.

The network $G(\omega)$ could in principle be any of the variants described above in \cref{sec:methods:variants},
but for the thesis at hand, only the initial network from \cref{sec:methods:basicSetup} was used.

\subsubsection{Combination of mean squared error and TAD-score-based loss} \label{sec:methods:score_loss}
Formally, the the following optimization task can be defined by means of a combination between MSE and TAD-score based loss:
\begin{equation}
 \omega^* = \mathit{arg\,min}_\omega (  \lambda_\mathit{MSE} \frac{1}{N} \sum_{s=0}^N (M_s - T_s)^2
                                                    + \lambda_\mathit{score} \frac{1}{N} \sum_{s=0}^N (\mathit{score}(M^\prime_s,ds) - \mathit{score}(T^\prime_s,ds))^2 \label{eq:methods:score_loss}
\end{equation}
where $M_s$ is again the Hi-C submatrix predicted by the network $G(\omega)$ for sample $s$ and $T_s$ is the corresponding true Hi-C submatrix.

To compute the TAD-score-based loss $\mathit{score}(\cdot,\cdot)$, the predictions and true Hi-C matrices $M$ and $T$ in vector form (upper triangular part)
were first converted back to complete, symmetric Hi-C matrices $M^\prime,\; T^\prime$. 
Next, in a custom network layer, all diamonds with size $ds$ inside the submatrices of size $w$ were cut out using tensor slicing and the values inside the diamonds were reduced to their respective mean.
This yields score vectors -- more exactly, tensors with shape $(w - 2\,ds, 1)$.
After computing the latter for both predicted- and real Hi-C submatrices, the mean squared error between them was computed as usual and weighted with 
a user-selected loss weight $\lambda_\mathit{score}$, see \cref{eq:methods:score_loss}.

While it would also have been possible, and probably faster, to slice the outputs of the networks directly in vector form, 
this is rather unintuitive and was therefore not implemented for the thesis. 

\subsection{HiC-GAN approach}
\subsubsection{Modified Pix2Pix network}\label{sec:methods:cGAN_initial}
Several implementations of the original pix2pix network are publicly available, usually for the original image size of $256\times256$.
For the thesis at hand, implementation concepts from two tutorials \xxx were combined and the code was adapted to the given requirements.

Within the generator, two of the down- and upsampling layers inside the U-Net portion were made optional 
to allow processing smaller images of sizes $64\times64$ and $128\times128$, see \cref{fig:methods:GAN_arch:generator}.
Note that the generator (still) only supports square images with edge lengths that are powers of two and at least 64.
Furthermore, symmetry of the output was enforced by adding its transpose and multiplying by 0.5, cf. \cite{Fudenberg2020}.
The down- and upsampling layers shown in  \cref{fig:methods:GAN_arch:generator} are custom blocks 
detailed in \cref{fig:methods:GAN_arch:downsampling} and \ref{fig:methods:GAN_arch:upsampling}. 
All 2D-convolutions and 2D-deconvolutions had kernel size $(4,4)$ and were initialized with values drawn from a normal distribution with mean $\mu=0$ and
standard deviation $\sigma=0.02$.
Finally, the activation of the output layer was changed from tanh to sigmoid, because better results were observed
in conjunction with the given Hi-C matrices, probably because these contain no negative values by definition.

Compared to the original pix2pix setup, 
one downsampling layer was omitted in the discriminator for windowsize $w=\{256,128\}$ and another one for $w=64$.
This made the discriminator patches larger, especially for the smaller windowsizes,
cf. \cref{fig:methods:GAN_arch:discriminator}.
Symmetry was enforced after all convolutions in the same way as in the generator.
Kernel sizes and initializations for all 2D convolutions also were the same as for the generator,
and leaky ReLU-activations were used with parameter $\alpha=0.2$, as in all up- and downsampling layers.

Both the discriminator and the generator feature their own, trainable embedding network
to convert the conditional input, i.\,e. the chromatin feature data of shape $(3w, n)$,
into grayscale images of shape $(w,w,1)$. 
These networks will be discussed below in \cref{sec:methods:dnn-embedding}
and \cref{sec:methods:cnn-embedding}.

The loss function was implemented as shown in \ref{eq:improve:cGAN_loss}
with parameters $\lambda_\mathit{cGAN}=10^{-5}, \lambda_\mathit{MAE}=1.0, \lambda_\mathit{TV}=10^{-12}$.
Optimization was performed on minibatches of size 32,4 and 2 for windowsizes 64, 128 and 256, respectively, 
using the Adam optimizer with learning rate $2\cdot10^{-5}$ and $\beta_1=0.5$ for both generator and discriminator.

\begin{figure}[p]
    \tiny
    \import{figures/GAN_arch/}{generatorModel.pdf_tex}
    \caption{Adapted generator model from pix2pix} \label{fig:methods:GAN_arch:generator}
\end{figure}
\begin{figure}[p]
    \scriptsize
    \centering
    \import{figures/GAN_arch/}{discriminatorModel.pdf_tex}
    \caption{Adapted discriminator model from pix2pix} \label{fig:methods:GAN_arch:discriminator}
\end{figure}
\begin{figure}[p]
    \scriptsize
    \centering
    \import{figures/GAN_arch/}{downsampling_block.pdf_tex}
    \caption{downsampling block} \label{fig:methods:GAN_arch:downsampling}
\end{figure}
\begin{figure}[p]
    \scriptsize
    \centering
    \import{figures/GAN_arch/}{upsampling_block.pdf_tex}
    \caption{upsampling block} \label{fig:methods:GAN_arch:upsampling}
\end{figure}


\begin{enumerate}
 \item the number of trainable parameters
\end{enumerate}

\subsubsection{Using a DNN for 1D-2D embedding} \label{sec:methods:dnn-embedding}
In order to use the DNN described in \cref{sec:methods:basicSetup} as an embedding network
for the cGAN, only small amendments were required to adjust the input shapes,
i.\,e. to provide symmetric Hi-C matrices as grayscale images instead of the upper-triangular-vector representation
native to the DNN, see \cref{fig:methods:dnn-embedding}.
\begin{figure}[p]
    \scriptsize
    \centering
    \import{figures/GAN_arch/}{dnn_embedding.pdf_tex}
    \caption{embedding network, DNN} \label{fig:methods:dnn-embedding}
\end{figure}
The triu-reshape layer is a custom tensorflow network layer which simply generates an output tensor
of appropriate shape $(w,w)$ and sets its upper triangular part to the values given by the input vector.
Symmetrization was then performed by adding this tensor to its transpose and dividing the values on the diagonal by two.
Finally the required third axis was added to get the shape of a grayscale image.
The number of trainable parameters for the DNN embedding is shown in \cref{tab:methods:embedding_dnn_params}.
Note that all trainable parameters stem from the DNN here; the reshaping layers do not have any trainable parameters.
\begin{table}[hbp]
\centering
\begin{tabular}{cc}
\hline
\multicolumn{1}{l}{\textbf{windowsize}} & \multicolumn{1}{l}{\textbf{trainable weights}} \\ \hline
64                                      & \xxx                                        \\
128                                     & \xxx                                        \\
256                                     & \xxx                                        \\ \hline
\end{tabular}
\caption{trainable weights for embedding DNN}\label{tab:methods:embedding_dnn_params}
\end{table}
\subsubsection{Using a CNN for 1D-2D embedding} \label{sec:methods:cnn-embedding}
The convolutional embedding network consists of 8 convolutional blocks and a final 1D convolution layer, 
as shown in \cref{fig:methods:GAN_arch:embedding_network}.
Each of the convolution blocks start with a 1D convolution with kernel size 4, strides 1, padding ``same'' 
and ``L2'' kernel regularization with parameter $l=0.02$, followed by batch normalization and leaky ReLU activation
with parameter $\alpha=0.2$. 
The last 1D convolution consists of $w$ filters with kernel size 4, strides 3 and padding ``same'',
followed by sigmoid activation; this last convolution layer was not using kernel regularization.
All kernel weights of the 1D convolutions in the embedding network were initialized by a Xavier initializer.
\begin{figure}[p]
    \scriptsize
    \centering
    \import{figures/GAN_arch/}{embedding_network.pdf_tex}
    \caption{embedding network, CNN} \label{fig:methods:GAN_arch:embedding_network}
\end{figure}

For the three windowsizes $w=\{64,128,256\}$ used within this thesis, the embedding network shown in \cref{fig:methods:GAN_arch:embedding_network} 
contained about 4.2 to 4.3 million trainable weights, see \cref{tab:methods:embedding_cnn_params} for details.
\begin{table}[hbp]
\centering
\begin{tabular}{cc}
\hline
\multicolumn{1}{l}{\textbf{windowsize}} & \multicolumn{1}{l}{\textbf{trainable weights}} \\ \hline
64                                      & 4243968                                        \\
128                                     & 4260416                                        \\
256                                     & 4293312                                        \\ \hline
\end{tabular}
\caption{trainable weights for embedding CNN}\label{tab:methods:embedding_cnn_params}
\end{table}


\subsection{Hardware} \label{sec:methods:hardware}
For the thesis, three virtual machines were used to train the neural networks, see \cref{tab:appendix:hardware}.
All training for \cref{sec:results:cgan} was done on machine 2, 
computations for sections \ref{sec:initialDNNresults}, \ref{sec:results:wider-longer-etc} and \ref{sec:results:binsize_winsize}
were done on machine 3 without GPU and computations for sections \ref{sec:results:loss_functions} and \ref{sec:results:scorebased} were done on machine 2.

Training the DNN with perception- or score-based losses as well as the GAN was not reasonably possible without GPU.
For the GAN, it was found that GPU memory should not fall short of the given values (\cref{tab:appendix:hardware})
to avoid undue limitations on batchsizes and/or windowsizes.
\SI{20}{\giga\byte} of main memory, as in machine 2, were not enough to train the GAN at windowsize 64 for more than 80 epochs.
It could not be clarified throughout the thesis whether this was due to a memory leak in tensorflow or due to the chosen implementation.

Training samples were stored as tensorflow tfrecords (on the fly at runtime) and a custom pipeline including a shuffle buffer and prefetching 
was employed to balance workload between CPUs and GPU.
This approach was found to be faster than generator-based approaches by a large margin.





