\section{Methods}
In the following sections, the technical details for the methods used within the thesis will be discussed.
First, input data and data preprocessing will be described, followed by an explanation of the quality metrics used to assess the predictions
of Hi-C matrices made throughout the thesis.
Next, sections \ref{sec:methods:dnn} and \ref{sec:methods:hicgan} will give details with regard to the two 
different approaches made in this thesis to advance the state of the art in predicting Hi-C matrices.
Finally, some special methods were employed to compare the results of this thesis to 
other approaches in the field, which will be described in \cref{sec:methods:comparison}.

\subsection{Input data} \label{sec:methods:input_data}
For the thesis at hand, data from human cell lines GM12878 and K562 was used.
The exact data sources and data preprocessing for the Hi-C matrices and ChIP-seq data 
will be outlined below.

\emph{Hi-C data} due to Rao et al. \cite{Rao2014} was downloaded 
in .hic format from Gene Expression Omnibus under accession key GSE63525.
Here, the quality-filtered ``combined\_30'' matrices were taken, which contain only high-quality reads from 
both replicates.

Next, matrices at \SI{5}{\kilo\bp} bin size were extracted and converted to cooler format using \texttt{hic2cool}
and subsequently coarsened to resolutions of 10, 25, 50 and \SI{100}{\kilo\bp} using \texttt{cooler coarsen},
see \cref{list:methods:hic2cool}.
Contrary to the work from Farr\'e et al. \cite{Farre2018a}, which is using ICE plus distance-based normalization,
and many others in the field which are using ICE- or KR-normalization, 
these matrices have not been normalized for the thesis at hand
because no benefit of doing so was found during the study project \cite{Krauth2020}.

\emph{ChIP-seq} data for 13 chromatin features and DNaseI-seq data was used, cf. \cref{tab:methods:csdata}.
Here, the data was downloaded in .bam format either via the ENCODE project \cite{Encode2012,Davis2017} 
or directly from the file server of the University of California (UCSC). 
In all cases, bam-files for replicate 1 and 2 were downloaded in their most recent versions (if applicable);
the UCSC- (wgEncode...) or GEO- (GSM...) identifiers are also given in \cref{tab:methods:csdata}.
For convenience, the pdf version of this document also provides download links for concrete files in \cref{sec:chromFeat_download_links}.
\begin{table}[ht!]
\centering
 \begin{tabular}{lll}
 \hline
  \multicolumn{1}{c}{\multirow{2}{*}{\textbf{feature name}}} & \multicolumn{2}{l}{\hspace*{18mm}\textbf{cell line}} \\
\multicolumn{1}{c}{}                                       & GM12878             & K562             \\ \hline
  CTCF & GSM733752 & GSM733719\\
  DNaseI & wgEncodeEH000534 & wgEncodeEH000530\\
  H3k27ac & GSM733771 & GSM733656\\
  H3k27me3 &GSM733758 & GSM733658\\
  H3k36me3 &GSM733679 & GSM733714\\
  H3k4me1  &GSM733772 & GSM733692\\
  H3k4me2  &GSM733769 & GSM733651\\
  H3k4me3 &GSM733708 & GSM733680\\
  H3k79me2 &GSM733736	& GSM733653	\\
  H3k9ac &GSM733677 & GSM733778\\
  H3k9me3 &GSM733664 & GSM733776	\\
  H4k20me1 &GSM733642 & GSM733675\\
  Rad21 &	wgEncodeEH000749 & wgEncodeEH000649\\
  Smc3 & 	wgEncodeEH001833 & wgEncodeEH001845\\ \hline
 \end{tabular}
 \caption{Chromatin features used for the thesis} \label{tab:methods:csdata}
\end{table}

After downloading, the bam files were converted to bigwig format, which was found more convenient to handle, and the replicates were merged into 
one bigwig file by taking the mean, as in the study project \cite{Krauth2020}. 
Pseudocode for the full conversion process is given in \cref{list:methods:bamtobigwig}.

The choice of chromatin features is widely in line with the work by Zhang et al. \cite{Zhang2019}; 
besides structural proteins like CTCF and Cohesin subcomponents RAD21 and SMC3, active/passive markers are used as well.

\subsection{Sample generation process for the dense neural network} \label{sec:methods:sample_gen}
With regard to sample generation, this thesis widely follows the sliding window approach proposed by Farr\'e et al. \cite{Farre2018a},
yet with an important difference, as will be outlined in the following paragraphs.

First, all chromatin features were binned to bin size $b_\mathit{feat}$ by splitting each chromosome of size $cs$ into 
$l_\mathit{feat}=\left \lceil{\frac{cs}{b_\mathit{feat}}}\right \rceil$ non-overlapping bins of size $b_\mathit{feat}$
and taking the mean feature signal value within the genomic region belonging to each bin.
All $n$ chromatin factors were processed in this way and then stacked into a $(l_\mathit{feat},n)$-shaped array.
The number of chromatin features was constant for all investigations within this thesis, $n=14$ (cf.~\cref{sec:methods:input_data}),
except for the comparisons with the method by Farr\'e et al., where 49 chromatin features from D. Melanogaster were used, cf. \cref{sec:methods:comparison}.

It should be noted that the way of feature binning described in the paragraph above constitutes a major difference to the proposal by Farr\'e et al. \cite[p.~9]{Farre2018a}.
In their work, values are assigned to each bin according to the percentage of basepairs in the respective bin covered by statistically significant enrichments of the respective feature,
where a value of one means 100\% coverage and a value of zero means the feature is not present in the respective bin at all.

Separate Hi-C matrices for each chromosome were derived from the cooler format as $(l_\mathit{mat}, l_\mathit{mat})$-shaped matrices, 
$l_\mathit{mat}=\left \lceil{\frac{cs}{b_\mathit{mat}}}\right \rceil$ being the number of bins in the given chromosome. 
Initially, $b_\mathit{feat} = b_\mathit{mat}$ was used, which leads to $l_\mathit{feat} = l_\mathit{mat}$, because $cs$ is a constant for a given chromosome.
While this is also a different process compared to the method used by Farr\'e et al. \cite[p.~8f]{Farre2018a}, it should lead to the same outcome --
apart from the fact that non-normalized matrices have been used here, cf. \cref{sec:methods:input_data}.

The same sliding window approach as proposed by Farr\'e et al. \cite{Farre2018a} was then applied to generate training samples 
from the chromatin feature array and matrix array for usage with the neural networks $G$ described below.
Here, subarrays of size $(3w_\mathit{mat}, n)$ were cut out from the feature array 
such that the $i$-th training sample corresponded to the subarray containing the columns $i,\,i+1,\,i+2,\,\dots,\,i+3w_\mathit{mat}$ of the full array. 
Sliding the window along the array with stepsize one obviously yields $N=l-3w_\mathit{mat}+1$ training samples.
The corresponding Hi-C matrices were then cut out along the diagonal of the original matrix 
as submatrices with corner indices [$j,\;j$], [$j,\;j+w_\mathit{mat}$], [$j+w_\mathit{mat},\;j+w_\mathit{mat}$], [$j+w_\mathit{mat},\;j$] in clockwise order, where $j=i+w_\mathit{mat}$.
The idea here is that the first $0,\,1,\,\dots \,w_\mathit{mat}$ columns of each feature sample form the left flanking region of the training matrix, 
the next $w_\mathit{mat}+1,\,w_\mathit{mat}+2,\,\dots \,2w_\mathit{mat}$ correspond to the matrix' region and the last $2w_\mathit{mat}+1,\,2w_\mathit{mat}+2,\,\dots \,3w_\mathit{mat}$ columns form the right flanking region.
Since Hi-C matrices are symmetric by definition, only the upper triangular part of the submatrices was used, 
flattened into a $w\cdot (w+1)/2$ vector.

\Cref{fig:methods:sample_gen} exemplarily shows the sample generation process for a matrix of shape (16,16) with $w_\mathit{mat}=4$ and $n=3$ chromatin features.
In this case, five training samples would be generated -- the one encircled in green and four more to the right, as the window is sliding from left to right until the
right flanking region hits the feature array boundary.
\begin{figure}[hbp]
 \begin{minipage}{0.65\textwidth}
   \resizebox{\textwidth}{!}{
    \small
    \import{figures/}{sample_generation_process.pdf_tex}}
    \caption{Sample generation process}
    \label{fig:methods:sample_gen}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin feature array
    \item sliding direction for sample generation windows
    \item first 4 diagonals of a $(16, 16)$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item first $(4, 4)$ training matrix, corrsponding to feature window A-B-C
\end{enumerate}
 \end{minipage}
\end{figure}

The sample generation process for predicting (unknown) matrices was the same as for training,
except that no matrix window was generated.
Due to the sliding window approach, the output of the network is a set of overlapping submatrices along the main diagonal of the actual target Hi-C matrix.
To generate the final submatrix, all submatrices were added up in a position-aware fashion 
and finally all values were divided by the number of overlaps for their respective position.
\Cref{fig:methods:prediction} exemplarily shows the prediction process for $N=5$ samples with window size $w=4$ for a matrix of shape (16,16).
Note that the first and last $w$ bins in each row (matrix diagonal) always remain empty due to the flanking regions,
as do all bins outside the main diagonal and the first $w-1$ side diagonals.
To improve visualisation, all predicted matrices were scaled to value range 0...1000 after re-assembly and stored in cooler format for further processing.
Conveniently, cooler also supports storing only the upper triangular part of symmetric matrices, minimizing conversion effort for the data at hand.

\begin{figure}
 \begin{minipage}{0.65\textwidth}
   \resizebox{\textwidth}{!}{
    \small
    \import{figures/}{prediction_process2.pdf_tex}}
    \caption{Prediction process}
    \label{fig:methods:prediction}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin feature array
    \item sliding direction for predicted submatrices
    \item first 4 diagonals of a predicted $(16, 16)$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item first predicted $(4, 4)$ matrix
\end{enumerate}
\raggedright{gray backround colors symbolize number of overlapping predictions at that position (bright = 1, dark = 4)}
 \end{minipage}
\end{figure}

Generally, training samples were drawn from chromosomes 1, 2, 4, 7, 9, 11, 13, 14, 16, 17, 18, 20 and 22 of cell line GM12878, 
validation samples from chromosomes 6, 8, 12 and 15 of cell line GM12878 and test samples from chromosomes 3, 5, 10, 19 and 21 of cell line K562.
This approximately implements a 60:20:20 train:validation:test split, see \cref{tab:methods:samples} for details.
Reversing training/validation and test cell line, which differ strongly in sequencing depth within the Hi-C experiments, 
was tested briefly for the cGAN approach and seemed to work, see \cref{sec:appendix:reverseTrainTest}.
For comparability reasons, two cGANs were additionally trained on single chromosomes 14 and 17 of cell line GM12878,  cf. \cref{sec:methods:comparison}.
Also for comparability reasons, a DNN and a cGAN were trained and tested on data from \emph{Drosophila Melanogaster} embryonal cells, again refer to \cref{sec:methods:comparison}
for details.

Note that window size $w=80$ is likely not suitable for resolutions beyond \SI{50}{\kilo\bp}, because the number of training samples becomes too small
to train a network with more than seven million parameters, cf.~\cref{sec:methods:basicSetup}.
\begin{table}[hbp]
\centering
\begin{tabular}{lrrrrrrr}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{chrom.}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{length/bp}}} & \multicolumn{6}{c}{\textbf{samples at $\mathbf{w=80}$ and bin size...}}                                                                                                \\
\multicolumn{1}{c}{}                                     & \multicolumn{1}{c}{}                                    & \multicolumn{1}{c}{5k} & \multicolumn{1}{c}{10k} & \multicolumn{1}{c}{25k} & \multicolumn{1}{c}{50k} & \multicolumn{1}{c}{250k} & \multicolumn{1}{c}{500k} \\ \hline
1                                                        & 249250621                                               & 49612                  & 24687                   & 9732                    & 4747                    & 759                      & 260                      \\
2                                                        & 243199373                                               & 48401                  & 24081                   & 9489                    & 4625                    & 734                      & 248                      \\
4                                                        & 191154276                                               & 37992                  & 18877                   & 7408                    & 3585                    & 526                      & 144                      \\
7                                                        & 159138663                                               & 31589                  & 15675                   & 6127                    & 2944                    & 398                      & 80                       \\
9                                                        & 141213431                                               & 28004                  & 13883                   & 5410                    & 2586                    & 326                      & 44                       \\
11                                                       & 135006516                                               & 26763                  & 13262                   & 5162                    & 2462                    & 302                      & 32                       \\
13                                                       & 115169878                                               & 22795                  & 11278                   & 4368                    & 2065                    & 222                      &                          \\
14                                                       & 107349540                                               & 21231                  & 10496                   & 4055                    & 1908                    & 191                      &                          \\
16                                                       & 90354753                                                & 17832                  & 8797                    & 3376                    & 1569                    & 123                      &                          \\
17                                                       & 81195210                                                & 16001                  & 7881                    & 3009                    & 1385                    & 86                       &                          \\
18                                                       & 78077248                                                & 15377                  & 7569                    & 2885                    & 1323                    & 74                       &                          \\
20                                                       & 63025520                                                & 12367                  & 6064                    & 2283                    & 1022                    & 14                       &                          \\
22                                                       & 51304566                                                & 10022                  & 4892                    & 1814                    & 788                     &                          &                          \\
\multicolumn{2}{r}{$\mathbf{\sum}$ \textbf{train samples}}                          & \textbf{337986} & \textbf{167442}  & \textbf{65118}   & \textbf{31009}   & \textbf{3755}   & \textbf{808}                      \\
                                                         &                                                         &                        &                         &                         &                         &                          &                          \\
6                                                        & 171115067                                               & 33985                  & 16873                   & 6606                    & 3184                    & 446                      & 104                      \\
8                                                        & 146364022                                               & 29034                  & 14398                   & 5616                    & 2689                    & 347                      & 54                       \\
12                                                       & 133851895                                               & 26532                  & 13147                   & 5116                    & 2439                    & 297                      & 29                       \\
15                                                       & 102531392                                               & 20268                  & 10015                   & 3863                    & 1812                    & 172                      &                          \\
\multicolumn{2}{r}{$\mathbf{\sum}$ \textbf{valid. samples}}                  & \textbf{109819}    & \textbf{54433}       & \textbf{21201}    & \textbf{10124}     & \textbf{1262}         & \textbf{187}    \\
                                                         &                                                         &                        &                         &                         &                         &                          &                          \\
3                                                        & 198022430                                               & 39366                  & 19564                   & 7682                    & 3722                    & 554                      & 158                      \\
5                                                        & 180915260                                               & 35945                  & 17853                   & 6998                    & 3380                    & 485                      & 123                      \\
10                                                       & 135534747                                               & 26868                  & 13315                   & 5183                    & 2472                    & 304                      & 33                       \\
19                                                       & 59128983                                                & 11587                  & 5674                    & 2127                    & 944                     &                          &                          \\
21                                                       & 48129895                                                & 9387                   & 4574                    & 1687                    & 724                     &                          &                          \\
\multicolumn{2}{r}{$\mathbf{\sum}$ \textbf{test samples}}                                                                          & \textbf{123153}        & \textbf{60980}          & \textbf{23677}          & \textbf{11242}          & \textbf{1343}            & \textbf{314}             \\
                                                         &                                                         & \multicolumn{1}{l}{}   & \multicolumn{1}{l}{}    & \multicolumn{1}{l}{}    & \multicolumn{1}{l}{}    & \multicolumn{1}{l}{}     & \multicolumn{1}{l}{}     \\
\multicolumn{2}{r}{$\mathbf{\sum}$ \textbf{total samples}}                                                                         & \textbf{570958}        & \textbf{282855}         & \textbf{109996}         & \textbf{52375}          & \textbf{6360}            & \textbf{1309}            \\ \hline
\end{tabular}
\caption{Training, validation and test samples for sliding window approach} \label{tab:methods:samples}
\end{table}

\subsection{Sample generation for the cGAN} \label{sec:methods:sample_gen_cgan}
The samples for the \acrshort{cgan} were generated the same way as the samples for the dense neural network described 
in \cref{sec:methods:sample_gen}, with two exceptions.
First, Hi-C (sub-)matrices were not entered as vectors corresponding to their upper triangular part, 
but were instead taken as $(w, w, 1)$-shaped grayscale images with value range [0...1] in 32-bit floating point format.
Second, for the \acrshort{cgan} approach the input matrices need not only be square,
but their sizes also need to be powers of two. 
This requiredment is due to the connected up- and downsampling operations in the generator, 
which essentially are 2D-convolutions and transposed 2D-convolutions with strides two.
Within the thesis, window sizes of $w=\{64,128,256\}$ were used, see \cref{sec:results:cgan}.

\subsection{Generalization of feature binning} \label{sec:methods:inputBinning}
Most of the binning- and sample generation procedures described above 
also work for bin size relations $k=\frac{b_\mathit{mat}}{b_\mathit{feat}} \in \mathbb{N}^{>1}$.

To this end, the training matrices remained unchanged, i.\,e. $(l, l)$-shaped arrays, from which training submatrices of size  $(w_\mathit{mat}, w_\mathit{mat})$
were extracted. 
With $k \in \mathbb{N}^{>1}$, one bin on the matrix diagonal corresponded to $k$ bins of the feature array,
so the feature window size needed to be $k$ times the submatrix window size, $w_\mathit{feat} = k \cdot w_\mathit{mat}$.
Since the first layer of all neural networks used in this thesis was a 1D convolution,
this was achieved by setting the filter width and strides parameters of the (first) convolutional layer to $k$, leaving the rest of the network unchanged.
However, the number of bins along the matrix diagonal is generally not $k$ times the number of bins in the feature array,
see \cref{eq:methods:binning}.
\begin{equation}
 l_\mathit{feat} = \left \lceil{\frac{cs}{b_\mathit{feat}}}\right \rceil
                = \left \lceil{\frac{cs}{k \cdot b_\mathit{mat}}}\right \rceil 
                \not = k \cdot \left \lceil{\frac{cs}{ b_\mathit{mat}}}\right \rceil
                = k \cdot l_\mathit{mat} \label{eq:methods:binning}
\end{equation}

For the training process, this discrepancy was resolved by simply dropping the last training sample, 
if the feature window belonging to it had missing bins.
For the prediction process, the feature array was padded with zeros on its end.
This procedure ensured that no errors were introduced into the \emph{training} process by imputing values,
but kept the size of the \emph{predicted} matrix consistent with the actual chromosome size.

\Cref{fig:methods:sample_gen_generalized} shows the generalized training process with a $(16, 16)$-training matrix and $k=2$. 
If  $15\cdot b_\mathit{mat} + 1 \leq cs < 15\cdot b_\mathit{mat} + b_\mathit{feat}$ held for the chromosome size $cs$ in the example,
then the number of bins on the matrix diagonal would be $l_\mathit{mat}=16$, while the number of chromatin feature bins would be $l_\mathit{feat}=31 \not = 2 \cdot l_\mathit{mat}$.
\begin{figure}
 \begin{minipage}{0.65\textwidth}
   \resizebox{\textwidth}{!}{
    \small
    \import{figures/}{sample_generation_process_generalized.pdf_tex}}
    \caption{Generalized sample generation process f. $k=2$}
    \label{fig:methods:sample_gen_generalized}
 \end{minipage}\hfill
 \begin{minipage}{0.3\textwidth}
 \scriptsize
  \begin{enumerate}[label=\Alph*:,leftmargin=*]
   \raggedright
    \item left flanking region
    \item matrix region
    \item right flanking region
    \item chromatin features array ($k=2$)
    \item sliding direction for sample generation windows
    \item first 4 diagonals of a $(16, 16)$ Hi-C matrix, rotated \SI{45}{\deg} counterclockwise
    \item last $(4, 4)$ training matrix, corrsponding to feature window A-B-C; to be dropped/padded
\end{enumerate}
 \end{minipage}
\end{figure}
In this case, the 5th sample would be dropped for training,
while a column with zero bins -- symbolized by pink crosses in \cref{fig:methods:sample_gen_generalized} -- would be added to the feature array for prediction
so that the resulting matrix would still have the desired size of $(16, 16)$.

\subsection{Quality metrics for predicted Hi-C matrices} \label{sec:methods:metrics}
Assessing the quality of synthetically generated images is a long-standing problem, which has not yet been solved, see e\,g. \cite[p.\,19]{Wang2020}.
Within this thesis, distance-stratified Pearson correlations were computed between predicted and real matrices to measure the quality of the predictions.
While the suitability of suchlike correlations as a quality metric for Hi-C matrices seems debatable in general \cite{Yang2017}, 
distance stratified Pearson correlation is quite common in the field of Hi-C matrices and thus useful at least for comparisons with other approaches.

To compute the correlations, the $w$ bins at the left and right boundaries of the investigated matrices were first removed, because these never contain 
valid values due to the chosen sample generation approach, 
cf.~\cref{sec:methods:sample_gen}, \cref{fig:methods:prediction} and \cref{sec:methods:sample_gen_cgan}.
Next, the remaining values were grouped into vectors according to their genomic distance $d$, up to the maximum distance $w$, 
and then Pearson correlations $\rho$ were computed separately for each vector as usual, \cref{eq:pearson}.
Here, $\mathit{true}|_d$ and $\mathit{pred}|_d$ are the true and predicted values restricted to bin distance $d$, while
$\sigma(\mathit{true}|_d)$ and $\sigma(\mathit{pred}|_d)$ are the respective standard deviations and $cov(\cdot,\cdot)$ is the covariance.
\begin{equation}
 \rho(\mathit{true}|_d, \; \mathit{pred}|_d) = \frac{cov(\mathit{true}|_d, \mathit{pred}|_d)}{\sigma(\mathit{true}|_d)\; \sigma(\mathit{pred}|_d)} \label{eq:pearson}
\end{equation}
Since a numerically stable and efficient implementation for computing Pearson correlation is non-trivial, the corresponding function from pandas was used
for actual calculations \cite{pandasPearson}.
An obvious disadvantage of distance-stratified Pearson correlations is that they are undefined if either the predicted matrices or -- not observed in practice -- 
the true matrices have the same value, and thus zero standard deviation, for a certain distance.

In addition to the correlations themselves, the \acrfull{auc} was computed for all curves in each plot 
to obtain a single-valued, albeit very abstract quality metric.
In line with Zhang et al. \cite{Zhang2019}, trapez integration method was used for numerical area computations
and the values were normalized to the interval [-1, 1] by dividing through the maximum distance to allow for comparisons across experiments.
Furthermore, all Pearson correlation plots show the correlation between the ``true'' GM12878 matrix and the corresponding target matrix as a baseline,
i.\,e. the Pearson correlation that is obtained when simply using the corresponding GM12878 matrix and chromosome 
as a ``prediction'' for a given target matrix and chromosome.

In computer graphics, other similarity metrics like the structural similarity index (SSIM), peak signal-to-noise ratio (PSNR) or, especially for images generated by GANs, 
Fréchet inception distance (FID) seem more common than Pearson correlation.
However, their suitability for plots of Hi-C matrices seems not to have been investigated so far. 
Unlike Pearson correlation, SSIM and PSNR both depend on absolute values in the matrices to be compared, 
so that for example $\mathit{PSNR}(\mathbf{X}) \not=\mathit{PSNR}(k\cdot \mathbf{X})$ for $k\in\mathbb{R}^{>1}$ and $X$ a matrix of appropriate shape, 
which seems not optimal for a use case where structures are more interesting than actual values.
Computing FID would require reshaping outputs into images of shape (299, 299, 3) to meet Inception v3 specifications \cite{Szegedy2016}.
This is a non-trivial task for the given matrix plots,
where the longer edge -- defined by $\mathit{chromsize}/\mathit{binsize}$ -- is usually much longer 
and the shorter edge -- defined by the windowsize $w$ -- is usually shorter than 299 bins, or pixels, respectively.
While computing the FID for individual $(w,w)$-submatrices would be possible,
generalizing such results to the whole matrix seems without example in literature so far.

Since relying on one metric alone seemed problematic, selected areas of the test set were plotted against the true 
Hi-C matrices in these areas, see \cref{sec:methods:matrix_plots} below.
Note that visual comparison of predicted Hi-C matrices and true Hi-C matrices in this setting is strongly influenced by
the value range of the matrices, while the Pearson correlation is generally not.

\subsection{Matrix plots} \label{sec:methods:matrix_plots}
To allow for a visual comparison, selected areas of the predicted matrices were plotted against their true counterparts using pygenometracks \cite{LopezDelisle2020}.
Since it was impossible to plot the full test set every time due to the amount of figures needed, 
three exemplary regions were selected, chromosome 21, 19 and 5, 30 to \SI{40}{\mega\bp}, respectively. 
These regions were chosen because they feature both small and large TADs as well as nested TADs and subregions with very little chromatin feature signal values.

In general, most matrix plots additionally feature the sign of the first eigenvector in a purple track, named ``binarized PCA'' (values +1 or -1)
and the summarized value of all 14 chromatin features in a green track, named ``feature sum'', see e.g. \cref{fig:results:basic500}.
Feature sum was computed by binning all chromatin features at \SI{25}{\kilo\bp} and 
summing up the values across all features for each bin.
Eigenvectors were computed using \texttt{hicPCA} \cite{Wolff2018} and then the sign for each \SI{25}{\kilo\bp} bin was extracted using 
a simple python script. Finally, eigenvectors and feature sum were written out in bedgraph format, which pygenometracks can plot natively.

All tracks were plotted with an original resolution of 200\,dpi and a diagram width of \SI{150}{\mm}.
For the interaction counts in the Hi-C matrix plots, ``log1p'' transformation was applied and the true matrices were inverted,
i.\,e. reflected at the diagonal. 
No minimum or maximum values were set for plotting, but note that predicted matrices were scaled to value range 0...1000 
before they were stored, cf. \cref{sec:methods:sample_gen}.

\subsection{Dense neural network approach} \label{sec:methods:dnn}
In the following sections, the setup for the Dense Neural network inspired by Farr\'e et al. \cite{Farre2018a}
and its variations will be discussed in detail, starting with the basic setup that was subsequently modified.

\subsubsection{Basic setup} \label{sec:methods:basicSetup}
The basic setup for the dense neural network approach closely follows the proposal by Farr\'e et al. \cite{Farre2018a},
so a window size of $w=80$ and $b_\mathit{feat}=b_\mathit{mat} = \SI{10}{\kilo\bp}$ was initially used.
With these parameters, the network has a total of \SI{7486436}{} trainable weights in five trainable layers; 
one convolutional layer and four densely connected layers, \cref{fig:methods:basic_dnn}.
For brevity, the sigmoid activation after the 1D convolution and the ReLU activations after the Dense layers are not shown.
The dense layers all use a ``L2'' kernel regularizer and all dropout layers have a dropout probability of $10\%$.
\begin{figure}[htb]
    \small
    \centering
    \import{figures/}{DNN_basic_model.pdf_tex}
    \caption{Basic dense neural network with generalized feature binning}
    \label{fig:methods:basic_dnn}
\end{figure}

The training goal for the neural network $G$ is to find weights $\bm{\omega}^*$ for its five trainable layers 
such that the mean squared error $L_2$ between the predicted submatrices $\mathbf{M}_s = G_s(\bm{\omega})$ 
and the training submatrices $\mathbf{T}_s$ becomes minimal for all $N$ training samples $s \in (1,2,\dots, N)$. 
Here, $\mathbf{M}_s$ is given by the activations of the last dense layer, which are to be interpreted as the upper triangular 
part of a symmetric $(w, w)$-shaped Hi-C matrix.
Formally, one needs to optimize
\begin{equation}
 \bm{\omega}^* = \mathit{arg\,min}_{\bm{\omega}} L_2(\bm{\omega}) = 
        \mathit{arg\,min}_{\bm{\omega}} \frac{1}{N} \sum_{s=0}^N (\mathbf{M}_s - \mathbf{T}_s)^2 \label{eq:methods:nn-mse}
\end{equation} 
For the thesis at hand, \acrfull{sgd} with learning rate $10^{-5}$  was used to find $\bm{\omega}^*$,
except for some of the comparisons with other approaches described in \cref{sec:methods:comparison}.
Initial values $\bm{\omega}_\mathit{init}$ were drawn from a Xavier initializer, a uniform distribution with limits depending on the in- and output shapes.
Following \cite{Farre2018a}, optimization was performed on minibatches of 30 samples, assembled randomly from the $N$ training samples
to prevent location-dependent effects and improve generalization.
For training, the last batch was dropped, if $N/30 \not \in \mathbb{N}$. 

The network and its learning algorithm were implemented in python using tensorflow deep learning framework, partially with keras frontend \cite{Abadi2015,Chollet2015}.
All code is available under an open source license, see github repository \cite{Krauth2021b}. 
All computations were performed on virtual machines with different properties, partially with GPU assistance; see \ref{sec:appendix:hardware} for details.

\subsubsection{Modifying kernel size, number of filter layers and filters} \label{sec:methods:variants}
For the ``wider'' variant, the kernel size of the of the 1D convolutional layer was increased to $4k$ with strides $k$,
where $k=b_\mathit{mat}/b_\mathit{feat}$ is the relation beetween matrix- and feature bin size.
Mirror-type padding was used to maintain the output dimensions of the basic network, which had \SI{7486478}{} trainable weights
for $k=1$.

For the ``longer'' variant, three 1D-convolutional layers with 4, 8 and 16 filters 
were used in place of the basic network's single convolutional layer, cf. \cref{fig:methods:longer_dnn}.
This replacement was also made for the ``wider-longer''-variant, 
additionally increasing the respective kernel sizes to $4k$ (with strides $k$), 4 and 4, cf. \cref{fig:methods:longer_dnn} (right).
In both cases, the dropout rate was increased to 20\%.
The ``longer'' variant had \SI{9142665}{} trainable weights and the ``wider-longer'' variant had \SI{9143313}{} for $k=1$.

For the variant with generalized binning, features were binned at $b_\textrm{feat} = \SI{5}{\kilo\bp}$ while keeping the matrix bin size $b_\textrm{mat} = \SI{25}{\kilo\bp}$,
so the factor for the relation between the two bin sizes was $k=25/5=5$.
This yields input feature arrays of size $(3w\cdot k, n) = (3\cdot80\cdot5 , 14) = (1200, 14)$.
Replacing the first convolutional layer of the basic network by a 1D-convolutional filter with kernel size $k=5$ and strides $k=5$ without padding,
this input was again compressed to a $(3w, 1) = (240, 1)$-shaped tensor and fed into the flatten layer, cf~\cref{fig:methods:basic_dnn}. 
The rest of the network remained the same so that the number of trainable parameters increased only slightly to \SI{7486492}{}.
\begin{figure}[p]
    \small
    \centering
    \import{figures/}{DNN_longer.pdf_tex}
    \caption{``Longer'' and ``wider-longer'' variants for DNN}
    \label{fig:methods:longer_dnn}
\end{figure}

\subsubsection{Combination of mean squared error, perception loss and TV loss} \label{sec:methods:combined_loss}
For computing the combined \acrshort{mse}, perception- and TV-loss,
input features were first passed through the network as normal,
and mean squared error was computed on the predicted ``upper triangular'' vectors vs. the real vectors.
Next, both the output- and target vectors were converted to symmetric grayscale images by embedding them into 
$(w, w, 1)$-shaped tensors, where $w$ is again the window size.

For a pixel-image $\mathbf{y}$, total variation can be defined as the sum of the absolute differences for neighboring pixel-values, \cref{eq:methods:tv} \cite{Rudin1992}
\begin{equation}
 tv(\mathbf{y}) = \sum_{i,j}\sqrt{|y_{i+1,j} - y_{i,j}|^2 + |y_{i,j+1} - y_{i,j} |^2 } \label{eq:methods:tv}
\end{equation}
For efficiency, the tensorflow implementation from tf.image.total\_variation was used,
taking the sum across batches as loss value, as recommended in the tensorflow documentation \cite{TensorflowTV2020}.

For perception loss, the predicted images and the true images were first fed through a pre-trained \emph{VGG-16} network with fixed weights, truncated after the third convolution layer
in the fourth block (``block4\_conv3''), the last layer used by the influential work of Johnson et al. \cite{Johnson2016}.
The loss was then computed as mean squared error between the ``predicted'' and ``true'' output activations of the truncated VGG-16 network.

Let $\mathbf{M}_s=G_s(\bm{\omega})$ again be the output of the neural network $G$ described above, and $\mathbf{T}_s$ the true matrices for training samples $s$ in vector form,
and let $\mathbf{M}^\prime_s$ and $\mathbf{T}^\prime_s$ be their grayscale image counterparts as described above.
Furthermore, let $\mathit{tv}(\mathbf{x})$ be the total variation of image $\mathbf{x}$ and $\mathit{vgg}(\mathbf{x})$ the output of the perception loss network for image $\mathbf{x}$.
The optimization problem for the modified network was then formulated by means of \cref{eq:methods:combined_loss} to find weights $\bm{\omega}^*$ such that
\begin{equation}
 \bm{\omega}^* = \mathit{arg\,min}_\omega (  \lambda_\mathit{MSE} \frac{1}{N} \sum_{s=0}^N (\mathbf{M}_s - \mathbf{T}_s)^2 
                                                     + \lambda_\mathit{TV} \sum_{s=0}^N \mathit{tv}( \mathbf{M}^\prime_s) 
                                                     + \lambda_\mathit{VGG} \frac{1}{N} \sum_{s=0}^N (\mathit{vgg}(\mathbf{M}^\prime_s) - \mathit{vgg}(\mathbf{T}^\prime_s))^2 ) \label{eq:methods:combined_loss}
\end{equation}
Weight initialization for network $G$ and minibatching was done as described in \cref{sec:methods:basicSetup}.
The \emph{VGG-16} network and the corresponding weights were taken from the keras implementation pre-trained on \emph{ImageNet} \cite{deng2009}.
As a side effect, the usage of \emph{VGG-16} imposes the restriction $w \geq 32$ on the window size $w$, which is not problematic, since again $w=80$ was chosen for all experiments.

The network $G(\bm{\omega})$ could in principle be any of the variants described above in \cref{sec:methods:variants},
but for the thesis at hand, only the initial network from \cref{sec:methods:basicSetup} was used.

\subsubsection{Combination of mean squared error and TAD-score-based loss} \label{sec:methods:score_loss}
To optimize both for mean-squared error and a TAD-score-derived loss, the following optimization task was defined to find weights $\bm{\omega}^*$ such that
\begin{equation}
 \bm{\omega}^* = \mathit{arg\,min}_\omega (  \lambda_\mathit{MSE} \frac{1}{N} \sum_{s=0}^N (\mathbf{M}_s - \mathbf{T}_s)^2
                                                    + \lambda_\mathit{score} \frac{1}{N} \sum_{s=0}^N (\mathit{score}(\mathbf{M}^\prime_s,ds) - \mathit{score}(\mathbf{T}^\prime_s,ds))^2 \label{eq:methods:score_loss}
\end{equation}
where $\mathbf{M}_s$ is again the Hi-C submatrix predicted by the network $G(\bm{\omega})$ for sample $s$, and $\mathbf{T}_s$ is the corresponding true Hi-C submatrix.

To compute the TAD-score-based loss $\mathit{score}(\cdot,\cdot)$, the predictions and true Hi-C matrices $\mathbf{M}$ and $\mathbf{T}$ in vector form (upper triangular part)
were first converted back to complete, symmetric Hi-C matrices $\mathbf{M}^\prime,\; \mathbf{T}^\prime$. 
Next, in a custom network layer, all diamonds with size $ds$ inside the submatrices of size $w$ were cut out using tensor slicing and the values inside the diamonds were reduced to their respective mean.
This yields score vectors -- more exactly, tensors with shape $(w - 2\,ds, 1)$.
After computing the latter for both predicted- and real Hi-C submatrices, the mean squared error between them was computed as usual and weighted with 
a user-selected loss weight $\lambda_\mathit{score}$, see \cref{eq:methods:score_loss}.

While it would also have been possible, and probably faster, to slice the outputs of the networks directly in vector form, 
this is rather unintuitive and was therefore not implemented for the thesis. 

\subsection{Hi-cGAN approach} \label{sec:methods:hicgan}
In the following subsection, the setup and training process of the \acrshort{cgan} network developed for this thesis -- \emph{Hi-cGAN} -- and the respective
embeddings for the conditional input -- the chromatin features -- will be described in more detail.
First, the basic network setup based on the well-known \emph{pix2pix} \acrshort{cgan} architecture \cite{Isola2017} and the training process
are explained in \cref{sec:methods:cGAN_initial}, then the respective embedding networks are discussed in subsections \ref{sec:methods:dnn-embedding},
\ref{sec:methods:cnn-embedding} and \ref{sec:methods:mixed-embedding}.

\subsubsection{Modified pix2pix network}\label{sec:methods:cGAN_initial}
Several implementations of the original \emph{pix2pix} network \cite{Isola2017} are publicly available, usually for the original image size of $256\times256$.
For the thesis at hand, implementation concepts from two tutorials \cite{tfpix2pix2020, brownlee2019} were combined and the code was adapted to the given requirements.

Within the generator, two of the down- and upsampling layers inside the U-Net portion were made optional 
to allow processing smaller images of sizes $64\times64$ and $128\times128$, see \cref{fig:methods:GAN_arch:generator}.
Note that the generator (still) only supports square images with edge lengths that are powers of two and at least $2^6=64$.
Furthermore, symmetry of the output images was enforced by adding their transpose and multiplying by 0.5, cf. \cite{Fudenberg2020}.
The down- and upsampling layers shown in  \cref{fig:methods:GAN_arch:generator} are custom blocks 
detailed in \cref{fig:methods:GAN_arch:downsampling} and \ref{fig:methods:GAN_arch:upsampling}. 
All 2D-convolutions and 2D-deconvolutions had kernel size $(4,4)$ and were initialized with values drawn from a normal distribution with mean $\mu=0$ and
standard deviation $\sigma=0.02$.
Finally, the output layer of the generator was changed from tanh- to sigmoid activation.
This empirically showed better results, probably because the training- and test matrices were scaled to 0...1, which 
is also the value range of the sigmoid function.

Compared to the original \emph{pix2pix} setup, 
one downsampling layer was omitted in the discriminator for window size $w\in\{128,256,\dots\}$ and another one for $w=64$.
This made the discriminator patches larger, especially for the smaller window sizes,
cf. \cref{fig:methods:GAN_arch:discriminator}, and was empirically found to improve the results
slightly in the given application.
Symmetry was enforced after all convolutions in the same way as in the generator.
Kernel sizes and initializations for all 2D convolutions also were the same as for the generator,
and leaky ReLU-activations were used with parameter $\alpha=0.2$, as in all downsampling layers.

Both the discriminator and the generator featured their own, trainable embedding network
to convert the conditional input, i.\,e. the chromatin feature data of shape $(3w, n)$,
into grayscale images of shape $(w,w,1)$. 
These networks will be discussed below in \cref{sec:methods:dnn-embedding}
and \cref{sec:methods:cnn-embedding}.

The loss functions for the generator and discriminator were implemented as shown in equations (\ref{eq:improve:cGAN_loss}) to (\ref{eq:improve:disc_loss_total})
with parameters $\lambda_\mathit{adv}=1.0, \lambda_\mathit{MAE}=100.0, \lambda_\mathit{TV}=10^{-12}$ and $\lambda_D=0.5$.
Optimization was performed on minibatches of size 32, 8 and 2 for window sizes 64, 128 and 256, respectively, 
using Adam optimizers with learning rate $2\cdot10^{-5}$ for the generator, $10^{-6}$ for the discriminator and $\beta_1=0.5$ for both generator and discriminator.
The choice of batchsizes was partially dictated by memory limitations of the available GPUs, cf. \cref{sec:appendix:hardware}.

\begin{figure}[p]
    \tiny
    \import{figures/GAN_arch/}{generatorModel.pdf_tex}
    \caption{Adapted generator model from \emph{pix2pix}} \label{fig:methods:GAN_arch:generator}
\end{figure}
\begin{figure}[p]
    \scriptsize
    \centering
    \import{figures/GAN_arch/}{discriminatorModel.pdf_tex}
    \caption{Adapted discriminator model from \emph{pix2pix}} \label{fig:methods:GAN_arch:discriminator}
\end{figure}
\begin{figure}[p]
    \scriptsize
    \centering
    \import{figures/GAN_arch/}{downsampling_block.pdf_tex}
    \caption{Downsampling block} \label{fig:methods:GAN_arch:downsampling}
\end{figure}
\begin{figure}[p]
    \scriptsize
    \centering
    \import{figures/GAN_arch/}{upsampling_block.pdf_tex}
    \caption{Upsampling block} \label{fig:methods:GAN_arch:upsampling}
\end{figure}


\begin{enumerate}
 \item the number of trainable parameters \xxx
\end{enumerate}

\subsubsection{Using a DNN for feature embedding} \label{sec:methods:dnn-embedding}
In order to use the DNN described in \cref{sec:methods:basicSetup} as an embedding network
for the \acrshort{cgan}, only small amendments were required to adjust the input shapes,
i.\,e. to provide symmetric Hi-C matrices as grayscale images instead of the upper-triangular-vector representation
native to the DNN, see \cref{fig:methods:dnn-embedding}.
\begin{figure}[p]
    \scriptsize
    \centering
    \import{figures/GAN_arch/}{dnn_embedding.pdf_tex}
    \caption{Embedding network, DNN} \label{fig:methods:dnn-embedding}
\end{figure}
The triu-reshape layer was a custom tensorflow network layer which generates an output tensor
of appropriate shape $(w,w)$ and sets its upper triangular part to the values given by the input vector.
Symmetrization was then performed by adding this tensor to its transpose and subtracting the values on the diagonal once,
because the diagonal was contained both in the upper triangular part of the matrix and its transpose.
Finally, the required third axis was added to get the shape of a grayscale image.
The number of trainable parameters for the DNN embedding is shown in \cref{tab:methods:embedding_network_params}.
Note that all trainable parameters stem from the DNN here; the reshaping layers do not have any trainable parameters.
\begin{table}[htbp]
\centering
\begin{tabular}{lrr}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{window size}}} & \multicolumn{2}{c}{\textbf{trainable weights}}    \\
\multicolumn{1}{c}{}                                     & \multicolumn{1}{c}{CNN} & \multicolumn{1}{c}{DNN} \\ \hline
64                                                       & 4243968                 & 5502796                 \\
128                                                      & 4260416                 & 16034732                \\
256                                                      & 4293312                 & 57877612                \\ \hline
\end{tabular}
\caption{Trainable weights for embedding networks}\label{tab:methods:embedding_network_params}
\end{table}

For the thesis, the DNN-embedding was used in two ways.
First, it was trained together with the rest of the \acrshort{cgan} with weight initialization as described in 
\cref{sec:methods:basicSetup} and \ref{sec:methods:cGAN_initial}.
Second, a DNN was pre-trained as described in \cref{sec:methods:basicSetup} with window size $w=64$
and the learned weights were transferred to the \acrshort{cgan} once before the start of the training process,
which was then continued as described in \cref{sec:methods:cGAN_initial}.
The results of the pre-training are visualized in \cref{sec:appendix:pretraining_results} (\cref{fig:results:DNN64_pearson});
the state after 250 epochs was the one transferred to the \acrshort{cgan}.

DNN embeddings were only used with window size $w=64$ due to the large number of parameters
to be trained at window sizes 128, 256 and higher.

\subsubsection{Using a CNN for feature embedding} \label{sec:methods:cnn-embedding}
The convolutional embedding network consisted of 8 convolutional blocks and a final 1D convolution layer, 
as shown in \cref{fig:methods:GAN_arch:embedding_network}.
Each of the convolution blocks started with a 1D convolution with kernel size 4, strides 1, padding ``same'' 
and ``L2'' kernel regularization with parameter $l=0.02$, followed by batch normalization and leaky ReLU activation
with parameter $\alpha=0.2$. 
The last 1D convolution consisted of $w$ filters with kernel size 4, strides 3 and padding ``same'',
followed by sigmoid activation; this last convolution layer was not using kernel regularization.
All kernel weights of the 1D convolutions in the embedding network were initialized by a Xavier initializer.
\begin{figure}[p]
    \scriptsize
    \centering
    \import{figures/GAN_arch/}{embedding_network.pdf_tex}
    \caption{Embedding network, CNN} \label{fig:methods:GAN_arch:embedding_network}
\end{figure}

In the CNN-embedding, the number of trainable parameters was constant for all layers, except for the last convolutional layer, where the number
of convolutional filters was equal to the window size, cf. \cref{fig:methods:GAN_arch:embedding_network}. 
For the three window sizes $w=\{64,128,256\}$ used within this thesis, the CNN-embedding network 
contained about 4.2 to 4.3 million trainable weights, see \cref{tab:methods:embedding_network_params} for details.

\subsubsection{Mixed embedding for generator and discriminator} \label{sec:methods:mixed-embedding}
In the mixed setting, the dense neural network described above in \cref{sec:methods:dnn-embedding} was used as  
embedding network for the generator and the CNN described in \cref{sec:methods:cnn-embedding} was used as embedding
network for the discriminator.
The mixed setting was used both without and with weight transfer for its DNN embedding.
In the latter case, again the weights of the DNN were replaced by the ones of the same pre-trained DNN as in \cref{sec:methods:dnn-embedding}
 before the training process started.

\subsection{Comparison with other approaches} \label{sec:methods:comparison}
\textbf{Comparison with HiC-Reg}\\
For comparison with \emph{HiC-Reg}, stored predictions for cell line K562, chromosomes 14 and 17 were downloaded from Zenodo \cite{ShiluZhang2019,ShiluZhang2019a}.
Here, results from the ``WINDOW'' and ``MULTICELL'' approaches were selected, whereby the ``WINDOW'' method was using training data from GM12878, chromosome 14 or 17, 
while the MULTICELL approach was using chromatin feature data from cell lines GM12878, HMEC, HUVEC, K562(!) and NHEK with training matrices from GM12878, chromosome 14 or 17.
These results were deemed to offer the best comparability to the \acrshort{cgan}- and DNN approaches of this thesis.
The downloaded data is in text format and was first converted into cooler format, whereby a surprising sparsity of the data was noted.
For chromosome 14, only \SI{42.4}{\percent} and for chromosome 17, only \SI{59.9}{\percent} of all possible interacting pairs 
for the chosen window size $w=200$ at bin size $b=5000$ were contained in the datasets, 
and no interacting pairs with predicted interaction counts below 0.01 were found in the data. 
Instead, the value range was between 0.36 and 5.10 across all four datasets, with similar values for each dataset alone.
The missing pairs were found to be distributed all across the chromosome and all distance ranges, exept for distance zero, i.\,e. the Hi-C matrix diagonal, which was not contained in the datasets at all.
For the conversion to cooler format, the missing values were assumed to be zero.
Next, the cooler matrices were coarsened to bin size \SI{25}{\kilo\bp} to allow comparisons with the \acrshort{cgan}- and DNN results, using \texttt{cooler} \texttt{coarsen}.

For the comparison between \emph{HiC-Reg} and Hi-\acrshort{cgan}, two additional \acrshort{cgan} models with CNN embedding were trained on chromosome 14 and 17, respectively,
using (arbitariliy selected) chromosome 10 for validation. 
Due to the small number of training samples for in this setting, window size was set to $w=64$ and batch size to $bs=2$.
The rest of the training process remained the same, cf. \cref{sec:methods:cGAN_initial}.
Additionally, \emph{HiC-Reg} was compared to a \acrshort{cgan}-CNN model with window size $w=256$ and CNN embedding, trained on the typical training data set, 
which includes chromosomes 14 and 17 besides others, cf. sections \ref{sec:methods:cGAN_initial}, \ref{sec:methods:sample_gen} and \ref{sec:methods:sample_gen_cgan}.

When computing the distance-stratified Pearson correlations only from the available (non-zero) predicted values in the datasets \cite{ShiluZhang2019,ShiluZhang2019a},
the results for chromosome 17 showed very good accordance with the data published in the paper \cite[fig.\,10]{Zhang2019}, \cref{fig:methods:zhang_correlations_reconstructed},
allowing the conclusion that the correlation computations should in principle be comparable between this thesis and the paper by Zhang et al.
\begin{figure}[htbp]
 \begin{subfigure}{0.45\textwidth}
   \resizebox{\textwidth}{!}{
   \scriptsize
   \import{figures/randomforest/}{pearson_chr14_originalData_ZhangEtAl.pdf_tex}}
 \end{subfigure}\hfill
 \begin{subfigure}{0.45\textwidth}
  \resizebox{\textwidth}{!}{
   \scriptsize
   \import{figures/randomforest/}{pearson_chr17_originalData_ZhangEtAl.pdf_tex}}
 \end{subfigure}
\caption{Pearson correlations reconstructed from \cite{Zhang2019}} \label{fig:methods:zhang_correlations_reconstructed}
\end{figure}

For reference, training and prediction were re-done for the WINDOW approach and cell lines GM12878 $\rightarrow$ K562, chromosome 17.
With regard to chromatin features, the same bam files were taken as in \cref{sec:methods:input_data}, except for SMC3, which was replaced by TBP,
in line with the paper by Zhang et al. \cite{Zhang2019}. 
The bam files were then converted to text files using custom bash scripts and the \texttt{aggregateSignal} application provided by HiC-Reg, 
cf. \cref{list:methods:bam2bedgraph} and \ref{list:methods:bedgraph2hicreg}. 
For simplicity, only replicate 1 was used in all cases.
With regard to matrix data, cooler files were first prepared as described in \cref{sec:methods:input_data}.
Next, square root vanilla coverage correction was applied, the matrices were dumped into text files and a custom python script was used
to convert the text inputs to meet \emph{HiC-Reg}'s requirements, cf. \cref{list:methods:convertForHicReg_bash} and \ref{list:methods:convertForHicReg_python}.
For the rest of the training- and prediction process, the instructions in \emph{HiC-Reg}'s github repository were followed.
The resulting text files were converted back to cooler using another custom python script, see \cref{list:methods:hicreg2cool}.
The sparsity in the results was \SI{59.6}{\percent}, confirming that filtering occurs during the sample generation- or training process of HiC-Reg,
although it is not explicitly mentioned in the paper.
In terms of Pearson correlations -- here, including zero-values -- the recomputed results from replicate 1 were slightly better,
but overall very similar to the ones restored from the published datasets \cite{ShiluZhang2019,ShiluZhang2019a}. 

It seems strange that zero-values have not been considered when computing the correlations for the publication \cite{Zhang2019}.
Correctly predicted zero-values strongly increase the quality of the output matrices 
and should thus be included in the correlation computations.
Pearson correlations for the predictions in \cref{sec:results:comparison} were thus computed for full chromosomes, including zero-values.

To compare our results with the ones from Farr\'e et al. \cite{Farre2018a},
the corresponding Hi-C- and chromatin feature data for drosophila melanogaster were downloaded and pre-processed as described in the following paragraphs.

\emph{Hi-C data} from 16-18 hours old embryos was downloaded in text format from Schuettengruber et al. \cite{Schuettengruber2014} 
from the gene expression omnibus, accession GSE61471, for binsize \SI{10}{\kilo\bp} and subsequently converted to cooler format using a custom python script \cite[scripts/schuettengruberToCooler.py]{Krauth2021b}.

With respect to \emph{chromatin features}, raw reads from modEncode ChIP-seq experiments with 14-16 hours old embryos \cite{Roy2010} 
were obtained from sequence read archive (SRA) for 49 features in fastqsanger format, using the accession keys given in \cref{tab:appendix:features_farre}.
Note that while Farr\'e et al. state they have used 50 features, in fact only 49 unique features are specified in their paper, H3k4me1 being listed twice \cite[p.~9]{Farre2018a},
which explains why table \ref{tab:appendix:features_farre} contains 49 instead of the expected 50 entries.
After downloading, the reads were mapped to the drosophila melanogaster reference genome BDGP5/dm3 using bowtie2 mostly with default parameters, 
see \cref{list:methods:map_drosophila} for details,
and the resulting bam files were converted to bigwig format as described in \cref{sec:methods:input_data} and \cref{list:methods:bamtobigwig}.
While most experiments feature 2 to 4 replicates, only replicate 2 was considered for each chromatin feature to limit disk space usage and processing time.
The bigwig files were then used as inputs for the DNN and cGAN as described above in sections \ref{sec:methods:sample_gen}, \ref{sec:methods:sample_gen_cgan}, \ref{sec:methods:dnn}
and \ref{sec:methods:hicgan}. 

Here, the DNN was used mostly in its initial configuration at windowsize $w=80$,
but using a larger batchsize of 100, sample flipping as in \cite{Farre2018a} and the RMSprop optimizer with standard parameters, namely a learning rate of 0.001,
instead of the SGD optimizer used otherwise.
These changes were made following a comparison with the source code supposedly used for the paper \cite{Farre2018a}, see below.
Training was performed on samples from chromosome 2L, 2R and 3L, and chromosome X was used for validation.
All other parameters and settings of the DNN were left unchanged, cf. \cref{sec:methods:dnn}.

Farr\'e et al have not published raw data from which (predicted) Hi-C matrices and Pearson correlations could have been reconstructed,
but Pau Farr\'e kindly provided some of the non-public source code used for the paper \cite{Farre2018a},
following private communication.
After some minor bugfixes and adaptions to make the code work with python version 3, it was used to recompute the published results.
To this end, the same Hi-C matrix as mentioned above was dowloaded from GEO under accession GSE61471,
and the same 49 chromatin features were obtained from GEO, but in this case in *.gff3 file format required by the provided python application.
Note that in constrast to the bigwig files used for the DNN above, these files contain locations of statistically significant ``peaks''
and not mean read counts for the chromatin features at hand.
Originally, Farrè et al have been using chromosomes 2L, 2R, 3L and the first half of 3R for training, 
but after another simple modification to the source code provided, the training set was reduced to samples from chromosomes 2L, 2R and 3L,
as for the DNN above. This change did not visually impair the results. 
Validation samples were generated by a 80:20 split of the training/validation set, leaving Pau Farrè's implementation unchanged.
In disagreement with the publication \cite{Farre2018a}, the provided implementation is not using the SGD optimizer with batch size 30,
but instead the RMSprop optimizer with batch size 100.

Hi-cGAN was used with a windowsize $w=64$, which is the largest power of two 
smaller than the maximum windowsize of $w=80$ given by the data from Schuettengruber et al.
The batch size was set to 2, and training was performed on samples from chromosomes 2L and 2R, 
using chromosome 3L(!) for validation and 3R and X for testing.
All other parameters and settings were left unchanged, cf. \cref{sec:methods:hicgan}.

Since Farrè et al have reported Pearson correlations \emph{per position}, a custom python script 
was employed to compute such correlations also for the cooler matrices resulting from the DNN- and Hi-cGAN approaches \cite[scripts/corr\_per\_pos.py]{Krauth2021b}.
Additionally, the predicted matrices from Pau Farrè's python program were written out as numpy arrays and converted to cooler, 
adding a simple one-liner to the provided code. 
Distance-stratified Pearson correlations \emph{per distance} were then computed from the cooler matrices as usual.

In terms of matrix plots, the same cutout as in the paper \cite{Farre2018a} was used.
Here, the paper seems to contain an error, since the plotted excerpt does not coincide with Hi-C data from chromosome 3R, 15 to \SI{20}{\mega\bp}, as stated in the caption \cite[fig.~2]{Farre2018a},
but most likely stems from chromosome 3R, 16.75 to \SI{21.75}{\mega\bp}.
For comparability reasons, all matrix cutouts were plotted twice, once with the standard colormap of this thesis (``RdYlBu\_r'') and once
with the blue and red color scheme used in the publication \cite{Farre2018a}.
Additionally, the trackfile for pygenometracks was setup to obey the minimum and maximum values in the given cutout
because the automatic value range adjustment did not work well in this case.



\clearpage
